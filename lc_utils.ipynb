{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T23:11:43.207659Z",
     "start_time": "2019-09-09T23:11:43.189322Z"
    },
    "code_folding": [
     53,
     112
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lc_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lc_utils.py\n",
    "from typing import List, Union, Optional, Tuple\n",
    "import pandas as pd\n",
    "from j_utils import munging as mg\n",
    "\n",
    "\n",
    "def gen_datasets(today: str,\n",
    "                 valid_start: str,\n",
    "                 base_loan_info: pd.DataFrame,\n",
    "                 eval_loan_info: pd.DataFrame,\n",
    "                 target: Union[str, List[str]],\n",
    "                 doneness: float = .95,\n",
    "                 stat_adj: bool = True,\n",
    "                 oldest: Optional[str] = None,\n",
    "                 valid_end: Optional[str] = None,\n",
    "                 verbose: bool = False,\n",
    "                 impute: bool = False,) -> Tuple:\n",
    "    #                 old_and_done: bool = False\n",
    "    '''\n",
    "    makes train_x, train_y, valid_x, valid_y, train_ids, valid_ids\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        today: string, marks the date. Training data is loans issued btwn @oldest until @today that have\n",
    "            an end_d < @today\n",
    "        valid_start: string, date to start validation set from. Must be greater than today and not the same year and month\n",
    "        base_loan_info: the pandas dataframe of loan info (e.g. X)\n",
    "        eval_loan_info: the pandas dataframe of target, other eval metrics (e.g. one or more of the columns is y)\n",
    "        target: define the target column from eval_loan_info\n",
    "        doneness: maturity time or maturity paid (or stat_adj versions) must be >= than this number\n",
    "        stat_adj: True or False, choosing whether to use status adjusted values. Default is True\n",
    "        oldest: Will not use loans that were issued before this date\n",
    "        valid_end: Will not include loans greater than this date in the validation set\n",
    "        verbose: for hyperlearn impute. Should be moved outside of this function\n",
    "        impute: for hyperlearn impute. Should be moved outside of this function\n",
    "    '''\n",
    "    \n",
    "    done_statuses = ['paid', 'charged_off', 'defaulted']\n",
    "    today = pd.to_datetime(today)\n",
    "    valid_start = pd.to_datetime(valid_start)\n",
    "    assert (today < valid_start - pd.to_timedelta(valid_start.day-1, unit='d')),'valid_start must be greater than today and not the same year and month'\n",
    "    \n",
    "    # cut loans to required doneness\n",
    "    if stat_adj:\n",
    "        eval_loan_info_mask = eval_loan_info.eval('maturity_time_stat_adj >= @doneness or '\n",
    "                                              'maturity_paid_stat_adj >= @doneness or '\n",
    "                                              'loan_status == @done_statuses')\n",
    "    else:\n",
    "        eval_loan_info_mask = eval_loan_info.eval('maturity_time >= @doneness or '\n",
    "                                              'maturity_paid >= @doneness or '\n",
    "                                              'loan_status == @done_statuses')\n",
    "    \n",
    "    # specify date bounds of train and valid sets\n",
    "    if oldest:\n",
    "        train_mask = eval_loan_info.eval(#'issue_d <= @today and '\n",
    "                                         'issue_d >= @oldest and '\n",
    "                                                 'end_d < @today') & eval_loan_info_mask\n",
    "    else:\n",
    "        train_mask = eval_loan_info.eval(#'issue_d <= @today and '\n",
    "                                                 'end_d < @today') & eval_loan_info_mask\n",
    "    if valid_end:\n",
    "        valid_mask = eval_loan_info.eval(\"issue_d >= @valid_start and \"\n",
    "                                         \"issue_d <= @valid_end\") & eval_loan_info_mask\n",
    "    else:\n",
    "        valid_mask = eval_loan_info.eval(\"issue_d >= @valid_start\") & eval_loan_info_mask\n",
    "        \n",
    "    train_x = base_loan_info.loc[train_mask]\n",
    "    valid_x = base_loan_info.loc[valid_mask]\n",
    "    \n",
    "    train_ids = train_x['id']\n",
    "    valid_ids = valid_x['id']\n",
    "    \n",
    "    assert len(train_x) == len(train_ids)\n",
    "    assert len(valid_x) == len(valid_ids)\n",
    "\n",
    "    if impute:\n",
    "        import hyperlearn.hyperlearn.impute.SVDImpute as hpl_imp\n",
    "        # setup for catboost\n",
    "        # a bit more data processing and nan handling for catboost\n",
    "        train_copy = train_x.copy()\n",
    "        valid_copy = valid_x.copy()\n",
    "\n",
    "        # get ready for hyperlearn svdimpute\n",
    "        train_copy, max_dict, min_dict, cats_dict, norm_dict = mg.train_hpl_proc(\n",
    "            train_copy, verbose=verbose)\n",
    "        valid_copy = mg.val_test_hpl_proc(\n",
    "            valid_copy, train_copy, max_dict, min_dict, cats_dict, verbose=verbose)\n",
    "\n",
    "        # fit to train\n",
    "        S, VT, mean, std, mins, standardise = hpl_imp.fit(train_copy.values)\n",
    "\n",
    "        # impute on train\n",
    "        train_svdimp = hpl_imp.transform(\n",
    "            train_copy.values, S, VT, mean, std, mins, standardise)\n",
    "        train_svdimp = pd.DataFrame(train_svdimp)\n",
    "        train_svdimp.index = train_copy.index\n",
    "        train_svdimp.columns = train_copy.columns\n",
    "\n",
    "        # impute on test\n",
    "        valid_svdimp = hpl_imp.transform(\n",
    "            valid_copy.values, S, VT, mean, std, mins, standardise)\n",
    "        valid_svdimp = pd.DataFrame(valid_svdimp)\n",
    "        valid_svdimp.index = valid_copy.index\n",
    "        valid_svdimp.columns = valid_copy.columns\n",
    "\n",
    "        # imputing changes some ids. Make the ids the originals again.\n",
    "        train_svdimp['id'] = train_ids\n",
    "        valid_svdimp['id'] = valid_ids\n",
    "        \n",
    "        train_x = train_svdimp\n",
    "        valid_x = valid_svdimp\n",
    "\n",
    "    if type(target) == str:\n",
    "        target = [target]\n",
    "    target = ['id'] + target\n",
    "    \n",
    "    train_y = eval_loan_info.loc[train_mask, target]\n",
    "    valid_y = eval_loan_info.loc[valid_mask, target]\n",
    "        \n",
    "    assert len(train_x) == len(train_ids) == len(train_y)\n",
    "    assert len(valid_x) == len(valid_ids) == len(valid_y)\n",
    "    \n",
    "    train_x = train_x.sort_values('id')\n",
    "    valid_x = valid_x.sort_values('id')\n",
    "    train_y = train_y.sort_values('id')\n",
    "    valid_y = valid_y.sort_values('id')\n",
    "    \n",
    "    assert (train_x['id'] != train_y['id']).sum() == 0\n",
    "    assert (valid_x['id'] != valid_y['id']).sum() == 0\n",
    "    \n",
    "    return train_x, train_y, valid_x, valid_y, train_ids, valid_ids\n",
    "\n",
    "\n",
    "# make a crude test set for now\n",
    "\n",
    "\n",
    "def get_split_date(df, date_column, quantile):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/31018622/pandas-quantile-function-for-dates\n",
    "    Get the date on which to split a dataframe for timeseries splitting\n",
    "    Adjusted coerce param to errors since SO is old.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. convert date_column to datetime (useful in case it is a string)\n",
    "    # 2. convert into int (for sorting)\n",
    "    # 3. get the quantile\n",
    "    # 4. get the corresponding date\n",
    "    # 5. return, pray that it works\n",
    "\n",
    "    quantile_date = pd.to_datetime(df[date_column], errors='raise').astype(\n",
    "        'int64').quantile(q=quantile)  # .astype('datetime64[ns]')\n",
    "\n",
    "    return pd.to_datetime(quantile_date)\n",
    "\n",
    "\n",
    "def split_out_traintestable_loans(df, eval_df, oldness_thrsh=.9):\n",
    "    '''Can train/test on loans that pass the oldness_thrsh or have status paid/defaulted/charged_off'''\n",
    "    old_enough_ids = eval_df[(eval_df['maturity_time_stat_adj'] >= oldness_thrsh) |\n",
    "                             (eval_df['maturity_paid_stat_adj'] >= oldness_thrsh) |\n",
    "                             (eval_df['loan_status'].isin(['paid', 'defaulted', 'charged_off']))]['id'].unique()\n",
    "    df = df[df['id'].isin(old_enough_ids)]\n",
    "    eval_df = eval_df[eval_df['id'].isin(old_enough_ids)]\n",
    "    return df, eval_df\n",
    "\n",
    "\n",
    "def add_custom_lc_features(df):\n",
    "    # added features\n",
    "    df['monthly_inc'] = df['annual_inc'] / 12\n",
    "    df['dti_w_loan'] = (df['dti'] * df['monthly_inc'] +\n",
    "                        df['installment']) / df['monthly_inc']\n",
    "    df['delinq_to_monthly_inc'] = df['delinq_amnt'] / \\\n",
    "        df['monthly_inc']\n",
    "    df['tot_cur_bal_to_monthly_inc'] = df['tot_cur_bal'] / \\\n",
    "        df['monthly_inc']\n",
    "    df['loan_to_inc'] = df['loan_amount'] / \\\n",
    "        df['monthly_inc']\n",
    "    \n",
    "# Deprecation\n",
    "# def gen_datasets(today: str,\n",
    "#                  valid_start: str,\n",
    "#                  base_loan_info: pd.DataFrame,\n",
    "#                  eval_loan_info: pd.DataFrame,\n",
    "#                  target: Union[str, List[str]],\n",
    "#                  doneness: float = .95,\n",
    "#                  stat_adj: bool = True,\n",
    "#                  oldest: Optional[str] = None,\n",
    "#                  valid_end: Optional[str] = None,\n",
    "#                  verbose: bool = False,\n",
    "#                  impute: bool = False) -> Tuple:\n",
    "#     '''\n",
    "#     all loans from oldest until today are taken as train. All loans issued after today until valid_end are used for validation. Uses hyperlearn svd_impute to impute missing values. Returns the train and test datasets. target can be single colname or list of colnames.\n",
    "#     Will take all done loans as well (e.g. loan_status is paid, defaulted, charged_off)\n",
    "#     Checks that train x/y are same length and order. Does same for valid\n",
    "#     '''\n",
    "    \n",
    "#     # cut loans to required doneness\n",
    "#     if stat_adj:\n",
    "#         eval_loan_info = eval_loan_info[(eval_loan_info['maturity_time_stat_adj'] >= doneness) |\n",
    "#                                     (eval_loan_info['maturity_paid_stat_adj'] >= doneness) |\n",
    "#                                     (eval_loan_info['loan_status'].isin(['paid', 'charged_off', 'defaulted']))]\n",
    "#     else:\n",
    "#         eval_loan_info = eval_loan_info[(eval_loan_info['maturity_time'] >= doneness) |\n",
    "#                                     (eval_loan_info['maturity_paid'] >= doneness) |\n",
    "#                                     (eval_loan_info['loan_status'].isin(['paid', 'charged_off', 'defaulted']))]\n",
    "    \n",
    "#     # specify date bounds of train and valid sets\n",
    "#     if oldest:\n",
    "#         train_ids = eval_loan_info[(eval_loan_info['issue_d'] <= today) & (\n",
    "#         eval_loan_info['issue_d'] >= oldest)]['id'].unique()\n",
    "#     else:\n",
    "#         train_ids = eval_loan_info[eval_loan_info['issue_d'] <= today]['id'].unique()\n",
    "#     if valid_end:\n",
    "#         valid_ids = eval_loan_info[(eval_loan_info['issue_d'] >= valid_start) & (\n",
    "#             eval_loan_info['issue_d'] <= valid_end)]['id'].unique()\n",
    "#     else:\n",
    "#         valid_ids = eval_loan_info[(\n",
    "#             eval_loan_info['issue_d'] >= valid_start)]['id'].unique()\n",
    "        \n",
    "#     train_x = base_loan_info[base_loan_info['id'].isin(train_ids)]\n",
    "#     valid_x = base_loan_info[base_loan_info['id'].isin(valid_ids)]\n",
    "    \n",
    "#     assert len(train_x) == len(train_ids)\n",
    "#     assert len(valid_x) == len(valid_ids)\n",
    "\n",
    "#     if impute:\n",
    "#         import hyperlearn.hyperlearn.impute.SVDImpute as hpl_imp\n",
    "#         # setup for catboost\n",
    "#         # a bit more data processing and nan handling for catboost\n",
    "#         train_copy = train_x.copy()\n",
    "#         valid_copy = valid_x.copy()\n",
    "\n",
    "#         # get ready for hyperlearn svdimpute\n",
    "#         train_copy, max_dict, min_dict, cats_dict, norm_dict = mg.train_hpl_proc(\n",
    "#             train_copy, verbose=verbose)\n",
    "#         valid_copy = mg.val_test_hpl_proc(\n",
    "#             valid_copy, train_copy, max_dict, min_dict, cats_dict, verbose=verbose)\n",
    "\n",
    "#         # fit to train\n",
    "#         S, VT, mean, std, mins, standardise = hpl_imp.fit(train_copy.values)\n",
    "\n",
    "#         # impute on train\n",
    "#         train_svdimp = hpl_imp.transform(\n",
    "#             train_copy.values, S, VT, mean, std, mins, standardise)\n",
    "#         train_svdimp = pd.DataFrame(train_svdimp)\n",
    "#         train_svdimp.index = train_copy.index\n",
    "#         train_svdimp.columns = train_copy.columns\n",
    "\n",
    "#         # impute on test\n",
    "#         valid_svdimp = hpl_imp.transform(\n",
    "#             valid_copy.values, S, VT, mean, std, mins, standardise)\n",
    "#         valid_svdimp = pd.DataFrame(valid_svdimp)\n",
    "#         valid_svdimp.index = valid_copy.index\n",
    "#         valid_svdimp.columns = valid_copy.columns\n",
    "\n",
    "#         # imputing changes some ids. Make the ids the originals again.\n",
    "#         train_svdimp['id'] = train_ids\n",
    "#         valid_svdimp['id'] = valid_ids\n",
    "        \n",
    "#         train_x = train_svdimp\n",
    "#         valid_x = valid_svdimp\n",
    "\n",
    "#     if type(target) == str:\n",
    "#         target = [target]\n",
    "#     target = ['id'] + target\n",
    "    \n",
    "#     train_y = eval_loan_info[eval_loan_info['id'].isin(train_ids)][target]\n",
    "#     valid_y = eval_loan_info[eval_loan_info['id'].isin(valid_ids)][target]\n",
    "        \n",
    "#     assert len(train_x) == len(train_ids) == len(train_y)\n",
    "#     assert len(valid_x) == len(valid_ids) == len(valid_y)\n",
    "    \n",
    "#     train_x.sort_values('id', inplace=True)\n",
    "#     valid_x.sort_values('id', inplace=True)\n",
    "#     train_y.sort_values('id', inplace=True)\n",
    "#     valid_y.sort_values('id', inplace=True)\n",
    "    \n",
    "#     assert (train_x['id'] != train_y['id']).sum() == 0\n",
    "#     assert (valid_x['id'] != valid_y['id']).sum() == 0\n",
    "    \n",
    "#     return train_x, train_y, valid_x, valid_y, train_ids, valid_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
