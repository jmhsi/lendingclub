{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Justin Hsi\n",
    "# With usage of dtyping, able to combine all the pmt_history parts into this one notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part  1 of cleaning lending club payment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:05:20.022166Z",
     "start_time": "2018-02-21T00:05:19.923825Z"
    }
   },
   "outputs": [],
   "source": [
    "import dir_constants as dc\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from tqdm import tqdm_notebook\n",
    "import data_science.lendingclub.dataprep_and_modeling.modeling_utils.data_prep_new as data_prep\n",
    "\n",
    "store_path = '/home/justin/lendingclub.h5'\n",
    "\n",
    "data_path = '/home/justin/rsync_dl_rig/unzipped_lc_csvs/'\n",
    "files = os.listdir(data_path)\n",
    "filename = [file_ for file_ in files if file_.startswith('PMTHIST')]\n",
    "if len(filename) > 1:\n",
    "    print('you need to delete old pmt history files')\n",
    "else:\n",
    "    filename = filename[0]\n",
    "# con = sqlite3.connect(\"/home/justin/lendingclub.db3\")\n",
    "# chunksize = 1e6\n",
    "\n",
    "date_cols = ['RECEIVED_D', 'IssuedDate', 'MONTH', 'EarliestCREDITLine']\n",
    "\n",
    "month_dict = {\n",
    "    'JAN': '1-',\n",
    "    'FEB': '2-',\n",
    "    'MAR': '3-',\n",
    "    'APR': '4-',\n",
    "    'MAY': '5-',\n",
    "    'JUN': '6-',\n",
    "    'JUL': '7-',\n",
    "    'AUG': '8-',\n",
    "    'SEP': '9-',\n",
    "    'OCT': '10-',\n",
    "    'NOV': '11-',\n",
    "    'DEC': '12-'\n",
    "}\n",
    "\n",
    "\n",
    "def check_mem():\n",
    "    # These are the usual ipython objects, including this one you are creating\n",
    "    ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "    # Get a sorted list of the objects and their sizes\n",
    "    print(sorted([(x, sys.getsizeof(globals().get(x))) for x in globals() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:44:58.066927Z",
     "start_time": "2018-02-19T04:44:45.926192Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "check = pd.read_csv(f'{data_path}{filename}', nrows=2500000)\n",
    "obj_cols = check.select_dtypes(include=['object']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:44:58.709509Z",
     "start_time": "2018-02-19T04:44:58.068532Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inquiries6M\n",
      "['0' '1' '4' '5' '*' '8' '2' '3' nan '9' '6' '7' 0 1 3 4 2 5 8 6 9 7]\n",
      "EmploymentLength\n",
      "['< 1 year' '10+ years' '1 year' '6 years' '9 years' '3 years' '5 years'\n",
      " '7 years' '2 years' '8 years' '4 years' nan]\n"
     ]
    }
   ],
   "source": [
    "# set up category dtype where possible, keep track of cols with nans/stupid\n",
    "# cols: Inquiries6m has strings, nans, and ints. Dumb.\n",
    "stupid_cols = []\n",
    "dtype = {}\n",
    "for col in obj_cols.columns:\n",
    "    if col not in date_cols:\n",
    "        try:\n",
    "            vals = check[col].unique()\n",
    "            vals.sort()\n",
    "            dtype[col] = CategoricalDtype(vals)\n",
    "        except:\n",
    "            stupid_cols.append(col)\n",
    "            print(col)\n",
    "            print(check[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:44:58.732693Z",
     "start_time": "2018-02-19T04:44:58.711427Z"
    }
   },
   "outputs": [],
   "source": [
    "# manual fixes to existing categoricals:\n",
    "for k,v in dtype.items():\n",
    "    if k == 'APPL_FICO_BAND':\n",
    "        dtype[k] = CategoricalDtype(v.categories, ordered=True)\n",
    "    elif k == 'HomeOwnership':\n",
    "        pass\n",
    "    elif k == 'Last_FICO_BAND':\n",
    "        if list(v.categories)[-1] != '845-HIGH':\n",
    "            miss_low = list(v.categories[-2:][::-1])\n",
    "            rest = list(v.categories[:-2])\n",
    "            dtype[k] = CategoricalDtype(miss_low+rest, ordered=True)\n",
    "    elif k == 'PERIOD_END_LSTAT':\n",
    "        pass\n",
    "    elif k == 'State':\n",
    "        pass\n",
    "    elif k == 'VINTAGE':\n",
    "        dtype[k] = CategoricalDtype(v.categories, ordered=True)\n",
    "    elif k == 'grade':\n",
    "        dtype[k] = CategoricalDtype(v.categories, ordered=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:44:58.758639Z",
     "start_time": "2018-02-19T04:44:58.734011Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in check.columns:\n",
    "    if col not in dtype.keys():\n",
    "        if col not in date_cols and col != 'LOAN_ID' and col not in stupid_cols:\n",
    "            dtype[col] = np.float32\n",
    "del check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:46:36.258200Z",
     "start_time": "2018-02-19T04:44:58.760217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmt_hist=pd.read_csv(f'{data_path}{filename}', dtype=dtype, na_values='*') #dtype=dtype, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# changing dtypes and then renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:51:27.824098Z",
     "start_time": "2018-02-19T04:46:36.259881Z"
    }
   },
   "outputs": [],
   "source": [
    "# fixing dates to be sensible dtypes\n",
    "for col in date_cols:\n",
    "    pmt_hist[col] = pd.to_datetime(\n",
    "    pmt_hist[col].str[:3].replace(month_dict) +\n",
    "    pmt_hist[col].str[3:],\n",
    "    format='%m-%Y')\n",
    "    \n",
    "# changing to float32    \n",
    "pmt_hist['Inquiries6M'] = pmt_hist['Inquiries6M'].astype(np.float32)    \n",
    "\n",
    "# fixing up Employment Length to categorical\n",
    "# pmt_hist['EmploymentLength'] = pmt_hist['EmploymentLength'].fillna('MISSING')\n",
    "emp_len_dtype = CategoricalDtype(['< 1 year', '1 year', '2 years',\n",
    "                                  '3 years', '4 years', '5 years', '6 years',\n",
    "                                  '7 years', '8 years', '9 years', '10+ years'], ordered=True)\n",
    "pmt_hist['EmploymentLength'] = pmt_hist['EmploymentLength'].astype(emp_len_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:13.030466Z",
     "start_time": "2018-02-19T05:05:08.076199Z"
    }
   },
   "outputs": [],
   "source": [
    "# renaming columns ____________________________________________________________\n",
    "rename_col_dict = {\n",
    "    'LOAN_ID': 'loan_id',\n",
    "    'PBAL_BEG_PERIOD': 'outs_princp_beg',\n",
    "    'PRNCP_PAID': 'princp_paid',\n",
    "    'INT_PAID': 'int_paid',\n",
    "    'FEE_PAID': 'fee_paid',\n",
    "    'DUE_AMT': 'amt_due',\n",
    "    'RECEIVED_AMT': 'amt_paid',\n",
    "    'RECEIVED_D': 'pmt_date',\n",
    "    'PERIOD_END_LSTAT': 'status_period_end',\n",
    "    'MONTH': 'date',\n",
    "    'PBAL_END_PERIOD': 'outs_princp_end',\n",
    "    'MOB': 'm_on_books',\n",
    "    'CO': 'charged_off_this_month',\n",
    "    'COAMT': 'charged_off_amt',\n",
    "    'InterestRate': 'int_rate',\n",
    "    'IssuedDate': 'issue_date',\n",
    "    'MONTHLYCONTRACTAMT': 'monthly_pmt',\n",
    "    'dti': 'dti',\n",
    "    'State': 'addr_state',\n",
    "    'HomeOwnership': 'home_ownership',\n",
    "    'MonthlyIncome': 'm_income',\n",
    "    'EarliestCREDITLine': 'first_credit_line_date',\n",
    "    'OpenCREDITLines': 'open_credit_lines',\n",
    "    'TotalCREDITLines': 'total_credit_lines',\n",
    "    'RevolvingCREDITBalance': 'revol_credit_bal',\n",
    "    'RevolvingLineUtilization': 'revol_line_util',\n",
    "    'Inquiries6M': 'inq_6m',\n",
    "    'DQ2yrs': 'dq_24m',\n",
    "    'MonthsSinceDQ': 'm_since_dq',\n",
    "    'PublicRec': 'public_recs',\n",
    "    'MonthsSinceLastRec': 'm_since_rec',\n",
    "    'EmploymentLength': 'emp_len',\n",
    "    'currentpolicy': 'current_policy',\n",
    "    'grade': 'grade',\n",
    "    'term': 'term',\n",
    "    'APPL_FICO_BAND': 'fico_apply',\n",
    "    'Last_FICO_BAND': 'fico_last',\n",
    "    'VINTAGE': 'vintage',\n",
    "    'PCO_RECOVERY': 'recovs',\n",
    "    'PCO_COLLECTION_FEE': 'recov_fees',\n",
    "}\n",
    "\n",
    "pmt_hist.rename(columns=rename_col_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at columns with null in them, figure out what I want to do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:13.044410Z",
     "start_time": "2018-02-19T05:05:13.032273Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_null_cols(df):\n",
    "    cols_with_nulls = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            cols_with_nulls.append(col)\n",
    "    return cols_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:22.553569Z",
     "start_time": "2018-02-19T05:05:13.045624Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_with_nulls = find_null_cols(pmt_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:22.567904Z",
     "start_time": "2018-02-19T05:05:22.555247Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_isnull_col(cols_with_nulls, df):\n",
    "    '''cols_with_nulls is list of dataframe column names with nulls'''\n",
    "    for col in cols_with_nulls:\n",
    "        df[col+'_isnull'] = np.where(df[col].isnull(),1,0).astype(np.float32)\n",
    "    print('done making isnull columns for cols with nulls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:24.099592Z",
     "start_time": "2018-02-19T05:05:22.569434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done making isnull columns for cols with nulls\n"
     ]
    }
   ],
   "source": [
    "make_isnull_col(cols_with_nulls, pmt_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:24.311853Z",
     "start_time": "2018-02-19T05:05:24.101211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment history for 1646778 different loan ids\n"
     ]
    }
   ],
   "source": [
    "# Check how many unique loan ids\n",
    "print('payment history for', len(pmt_hist['loan_id'].unique()), 'different loan ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:24.344382Z",
     "start_time": "2018-02-19T05:05:24.313461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>outs_princp_beg</th>\n",
       "      <th>princp_paid</th>\n",
       "      <th>int_paid</th>\n",
       "      <th>fee_paid</th>\n",
       "      <th>amt_due</th>\n",
       "      <th>amt_paid</th>\n",
       "      <th>pmt_date</th>\n",
       "      <th>status_period_end</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>revol_line_util_isnull</th>\n",
       "      <th>inq_6m_isnull</th>\n",
       "      <th>dq_24m_isnull</th>\n",
       "      <th>m_since_dq_isnull</th>\n",
       "      <th>public_recs_isnull</th>\n",
       "      <th>m_since_rec_isnull</th>\n",
       "      <th>emp_len_isnull</th>\n",
       "      <th>vintage_isnull</th>\n",
       "      <th>recovs_isnull</th>\n",
       "      <th>recov_fees_isnull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54734</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>581.297913</td>\n",
       "      <td>247.802078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>Current</td>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54734</td>\n",
       "      <td>24418.701172</td>\n",
       "      <td>587.059814</td>\n",
       "      <td>242.040207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>Current</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54734</td>\n",
       "      <td>23831.642578</td>\n",
       "      <td>592.878784</td>\n",
       "      <td>236.221222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>Current</td>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54734</td>\n",
       "      <td>23238.763672</td>\n",
       "      <td>598.755432</td>\n",
       "      <td>230.344559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>2009-12-01</td>\n",
       "      <td>Current</td>\n",
       "      <td>2009-12-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54734</td>\n",
       "      <td>22640.007812</td>\n",
       "      <td>604.690369</td>\n",
       "      <td>224.409653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>829.099976</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Current</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_id  outs_princp_beg  princp_paid    int_paid  fee_paid     amt_due  \\\n",
       "0    54734     25000.000000   581.297913  247.802078       0.0  829.099976   \n",
       "1    54734     24418.701172   587.059814  242.040207       0.0  829.099976   \n",
       "2    54734     23831.642578   592.878784  236.221222       0.0  829.099976   \n",
       "3    54734     23238.763672   598.755432  230.344559       0.0  829.099976   \n",
       "4    54734     22640.007812   604.690369  224.409653       0.0  829.099976   \n",
       "\n",
       "     amt_paid   pmt_date status_period_end       date        ...         \\\n",
       "0  829.099976 2009-09-01           Current 2009-09-01        ...          \n",
       "1  829.099976 2009-10-01           Current 2009-10-01        ...          \n",
       "2  829.099976 2009-11-01           Current 2009-11-01        ...          \n",
       "3  829.099976 2009-12-01           Current 2009-12-01        ...          \n",
       "4  829.099976 2010-01-01           Current 2010-01-01        ...          \n",
       "\n",
       "   revol_line_util_isnull  inq_6m_isnull  dq_24m_isnull  m_since_dq_isnull  \\\n",
       "0                     0.0            0.0            0.0                1.0   \n",
       "1                     0.0            0.0            0.0                1.0   \n",
       "2                     0.0            0.0            0.0                1.0   \n",
       "3                     0.0            0.0            0.0                1.0   \n",
       "4                     0.0            0.0            0.0                1.0   \n",
       "\n",
       "   public_recs_isnull m_since_rec_isnull  emp_len_isnull  vintage_isnull  \\\n",
       "0                 0.0                1.0             0.0             0.0   \n",
       "1                 0.0                1.0             0.0             0.0   \n",
       "2                 0.0                1.0             0.0             0.0   \n",
       "3                 0.0                1.0             0.0             0.0   \n",
       "4                 0.0                1.0             0.0             0.0   \n",
       "\n",
       "  recovs_isnull recov_fees_isnull  \n",
       "0           1.0               1.0  \n",
       "1           1.0               1.0  \n",
       "2           1.0               1.0  \n",
       "3           1.0               1.0  \n",
       "4           1.0               1.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmt_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:05:52.692613Z",
     "start_time": "2018-02-19T05:05:24.345698Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist.to_hdf(store_path, key='pmt_hist_temp1', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:06:21.985898Z",
     "start_time": "2018-02-19T05:05:52.694483Z"
    }
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore(store_path)\n",
    "pmt_hist = store['pmt_hist_temp1']\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making sure columns make logical sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:06:22.200237Z",
     "start_time": "2018-02-19T05:06:21.987658Z"
    }
   },
   "outputs": [],
   "source": [
    "# There are 5 columns dealing with money: princp_paid, int_paid, fee_paid,\n",
    "# recovs, and recovs_fee. princp_paid + int_paid + fee_paid is sometimes short\n",
    "# of amt_paid. Be conservative and rewrite amt_paid to be sum of said 3.\n",
    "# Also make all_cash_to_inv = amt_paid + recovs - recov_fees\n",
    "# Fee paid is always positive, and by inspection it is money borrower pays out\n",
    "pmt_hist[\n",
    "    'amt_paid'] = pmt_hist['princp_paid'] + pmt_hist['int_paid'] + pmt_hist['fee_paid']\n",
    "pmt_hist['recovs'].fillna(0, inplace=True)\n",
    "pmt_hist['recov_fees'].fillna(0, inplace=True)\n",
    "pmt_hist[\n",
    "    'all_cash_to_inv'] = pmt_hist['amt_paid'] + pmt_hist['recovs'] - pmt_hist['recov_fees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:06:29.803125Z",
     "start_time": "2018-02-19T05:06:22.201612Z"
    }
   },
   "outputs": [],
   "source": [
    "# status_period_end ____________________________________________________________\n",
    "status_fix = {\n",
    "    'Current': 'current',\n",
    "    'Late (31-120 days)': 'late_120',\n",
    "    'Fully Paid': 'paid',\n",
    "    'Charged Off': 'charged_off',\n",
    "    'Default': 'defaulted',\n",
    "    'Late (16-30 days)': 'late_30',\n",
    "    'In Grace Period': 'grace_15',\n",
    "    'Issued': 'issued'\n",
    "}\n",
    "pmt_hist['status_period_end'].replace(status_fix, inplace=True)\n",
    "status_fix_cats = CategoricalDtype(list(pmt_hist['status_period_end'].unique()))\n",
    "pmt_hist['status_period_end'] = pmt_hist['status_period_end'].astype(status_fix_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:06:34.813764Z",
     "start_time": "2018-02-19T05:06:29.804784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# fix on a few bad rows where I think there is a mistaken amt_paid ____________\n",
    "pmt_hist.ix[(pmt_hist['pmt_date'].isnull() & pmt_hist['amt_paid'] > 0),\n",
    "            'amt_paid'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store part 1 of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T05:07:07.127141Z",
     "start_time": "2018-02-19T05:06:34.815677Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist.to_hdf(store_path, key='pmt_hist_temp1', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2, removing duplicate date rows (compressing granulairty of data to monthly, as that's the only real way I can work with it currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:03.699647Z",
     "start_time": "2018-02-21T00:05:28.138906Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist = pd.read_hdf(store_path, key='pmt_hist_temp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:03.788278Z",
     "start_time": "2018-02-21T00:06:03.701184Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_pd_NaT(x):\n",
    "    return type(x) == type(pd.tslib.NaT)\n",
    "\n",
    "v_test_pd_NaT = np.vectorize(test_pd_NaT)\n",
    "\n",
    "def find_dupe_dates(group):\n",
    "    return pd.to_datetime(group[group.duplicated('date')]['date'].values)\n",
    "\n",
    "def merge_dupe_dates(group, column_iloc_map, dtypes_map):\n",
    "    df_chunks = []\n",
    "    dupe_dates = find_dupe_dates(group)\n",
    "    # add the good rows\n",
    "    df_chunks.append(group[~group['date'].isin(dupe_dates)])\n",
    "    \n",
    "    #sum up duplicate rows\n",
    "    for date in dupe_dates:\n",
    "        problem_rows = group[group['date'] == date]\n",
    "        ori_index = problem_rows.index\n",
    "        keep_row = problem_rows.iloc[-1].to_dict()\n",
    "        keep_row['outs_princp_beg'] = problem_rows.ix[ori_index[0],column_iloc_map['outs_princp_beg']]\n",
    "        \n",
    "        summed = problem_rows.sum()\n",
    "        keep_row['princp_paid'] = summed['princp_paid']\n",
    "        keep_row['int_paid'] = summed['int_paid']\n",
    "        keep_row['fee_paid'] = summed['fee_paid']\n",
    "        keep_row['amt_due'] = summed['amt_due']\n",
    "        keep_row['amt_paid'] = summed['amt_paid']\n",
    "        keep_row['charged_off_amt'] = summed['charged_off_amt']\n",
    "        keep_row['recovs'] = summed['recovs']\n",
    "        keep_row['recov_fees'] = summed['recov_fees']\n",
    "            \n",
    "\n",
    "        df_chunks.append(pd.DataFrame(pd.Series(keep_row),columns=[ori_index[-1]]).T)\n",
    "    fixed = pd.concat(df_chunks)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    try:\n",
    "        return fixed.astype(dtypes_map)\n",
    "    except:\n",
    "        for col in fixed.columns:\n",
    "#             series = fixed[col].copy()\n",
    "# #             series[np.apply_along_axis(v_test_pd_NaT, 0, series)] = np.nan\n",
    "#             series = pd.to_datetime(series)\n",
    "            try:\n",
    "                fixed[col] = fixed[col].astype(dtypes_map[col])\n",
    "            except:\n",
    "                fixed[col] = pd.to_datetime(fixed[col])\n",
    "        return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:04.021726Z",
     "start_time": "2018-02-21T00:06:03.789729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646778 groups to iterate through (for each loan id)\n"
     ]
    }
   ],
   "source": [
    "print('{0} groups to iterate through (for each loan id)'.format(len(pmt_hist['loan_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:09.888141Z",
     "start_time": "2018-02-21T00:06:04.023677Z"
    }
   },
   "outputs": [],
   "source": [
    "loans_with_two_entries_in_same_month = pmt_hist[pmt_hist.duplicated(\n",
    "    ['loan_id', 'date'])]\n",
    "dup_date_ids = loans_with_two_entries_in_same_month['loan_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:18.200835Z",
     "start_time": "2018-02-21T00:06:09.889537Z"
    }
   },
   "outputs": [],
   "source": [
    "already_good = pmt_hist[~pmt_hist['loan_id'].isin(dup_date_ids)]\n",
    "needs_fixing = pmt_hist[pmt_hist['loan_id'].isin(dup_date_ids)]\n",
    "\n",
    "id_grouped = needs_fixing.groupby('loan_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:06:18.240075Z",
     "start_time": "2018-02-21T00:06:18.202629Z"
    }
   },
   "outputs": [],
   "source": [
    "column_iloc_map = {\n",
    "            col_name: pmt_hist.iloc[-1].index.get_loc(col_name)\n",
    "            for col_name in pmt_hist.columns.values\n",
    "        }\n",
    "dtypes_map = already_good.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:11:12.909838Z",
     "start_time": "2018-02-21T00:06:18.241695Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ec0ce7a8743ac8713b1d31e059f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fixed_dfs = []\n",
    "for ids, group in tqdm_notebook(id_grouped):\n",
    "    if ids in dup_date_ids:\n",
    "        fixed_dfs.append(merge_dupe_dates(group, column_iloc_map, dtypes_map))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:11:34.146364Z",
     "start_time": "2018-02-21T00:11:34.132090Z"
    }
   },
   "outputs": [],
   "source": [
    "all_dfs = [already_good]\n",
    "all_dfs.extend(fixed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:12:21.448823Z",
     "start_time": "2018-02-21T00:11:35.718468Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:14:45.649103Z",
     "start_time": "2018-02-21T00:14:06.278859Z"
    }
   },
   "outputs": [],
   "source": [
    "# store\n",
    "pmt_hist.to_hdf(store_path, key='pmt_hist_temp2', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3, Adding a row where money related values are 0 for some loans that are missing records in a month (for npv calculation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmt_hist = pd.read_hdf(store_path, key='pmt_hist_temp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T00:26:34.402259Z",
     "start_time": "2018-02-21T00:26:34.378995Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_closest_previous_record(ids, issue_d, first_date, actual_months, month):\n",
    "    '''This function finds the closest previous month that is in the group. \n",
    "    It is here to handle cases where a record of one month is missing, but the\n",
    "    record before that missing month is also missing.'''\n",
    "    offset = pd.DateOffset(months=-1)\n",
    "    prev_month = month + offset\n",
    "    if month < issue_d:\n",
    "        print(ids)\n",
    "        return first_date\n",
    "    elif prev_month in actual_months:\n",
    "        return prev_month\n",
    "    else:\n",
    "        find_closest_previous_record(ids, issue_d, first_date, actual_months, prev_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T02:06:15.388753Z",
     "start_time": "2018-02-21T01:13:03.541067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce3e12528ee4104a412deff451cb498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_dfs = []\n",
    "fixed_ids = []\n",
    "id_grouped = pmt_hist.groupby('loan_id')\n",
    "for ids, group in tqdm_notebook(id_grouped):\n",
    "    # Copy Paste finished below\n",
    "    issue_d = group['issue_date'].min()\n",
    "    first_date = group['date'].min()\n",
    "    last_date = group['date'].max()\n",
    "    expected_months = set(pd.DatetimeIndex(start=first_date, end=last_date, freq='MS'))\n",
    "    actual_months = set(group['date'])\n",
    "    to_make_months = list(expected_months.symmetric_difference(actual_months))\n",
    "    to_make_months.sort()\n",
    "    if len(to_make_months) > 1:\n",
    "        fixed_ids.append(ids)\n",
    "        months_to_copy = []\n",
    "        for month in to_make_months:\n",
    "            months_to_copy.append(find_closest_previous_record(ids, issue_d, first_date, actual_months, month))\n",
    "        copied = group[group['date'].isin(months_to_copy)].copy()\n",
    "        copied['amt_paid'] = 0.0\n",
    "        copied['date'] = to_make_months\n",
    "        copied['amt_due'] = np.where(copied['date'] < first_date, 0, copied['amt_due'])\n",
    "        fixed_dfs.append(pd.concat([group, copied]))\n",
    "    else:\n",
    "        pass\n",
    "#         already_good_dfs.append(group)\n",
    "#         if len(already_good_dfs) == chunksize:\n",
    "#             better_sized_already_good_dfs.append(pd.concat(already_good_dfs))\n",
    "#             already_good_dfs = []\n",
    "#         if n+1 == n_chunks: # if on the last chunk\n",
    "#             better_sized_already_good_dfs.append(pd.concat(already_good_dfs))\n",
    "#             already_good_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T02:26:10.727070Z",
     "start_time": "2018-02-21T02:26:04.845216Z"
    }
   },
   "outputs": [],
   "source": [
    "already_good_df = pmt_hist[~pmt_hist['loan_id'].isin(fixed_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T02:26:13.762111Z",
     "start_time": "2018-02-21T02:26:10.728322Z"
    }
   },
   "outputs": [],
   "source": [
    "all_dfs = [already_good_df] + [pd.concat(fixed_dfs)]\n",
    "pmt_hist = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T02:30:27.444798Z",
     "start_time": "2018-02-21T02:29:21.218130Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist.to_hdf(store_path, key='pmt_hist_ready', format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below was investigating to base my choices above on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmt_hist['unaccounted_rec_pmt_money'] = pmt_hist['amt_paid'] - (\n",
    "#     pmt_hist['princp_paid'] + pmt_hist['int_paid'] + pmt_hist['fee_paid'])\n",
    "\n",
    "# # Don't care about differences less than a cent _______________________________\n",
    "# pmt_hist['unaccounted_rec_pmt_money'] = np.where(\n",
    "#     pmt_hist['unaccounted_rec_pmt_money'] < 0.01, 0,\n",
    "#     pmt_hist['unaccounted_rec_pmt_money'])\n",
    "\n",
    "# # These should probably be Received_amt 0 because there is no received_d\n",
    "# pmt_hist[(pmt_hist['unaccounted_rec_pmt_money'] > 0) & (pmt_hist['PERIOD_END_LSTAT'] != 'Fully Paid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # home_ownership _______________________________________________________________\n",
    "# home_ownership_fix = {\n",
    "#     'admin_us': 'other',\n",
    "#     'mortgage': 'mortgage',\n",
    "#     'rent': 'rent',\n",
    "#     'own': 'own',\n",
    "#     'other': 'other',\n",
    "#     'none': 'none',\n",
    "#     'any': 'none'\n",
    "# }\n",
    "# pmt_hist['home_ownership'] = pmt_hist['home_ownership'].str.lower().replace(\n",
    "#     home_ownership_fix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # public_recs __________________________________________________________________\n",
    "# records_fix = {\n",
    "#     '*': 1\n",
    "# }  #leave nan as nan, but * had at least 1 from m_since_record\n",
    "# pmt_hist['public_recs'] = pmt_hist['public_recs'].replace(records_fix).astype(\n",
    "#     float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # fico_apply __________________________________________________________________\n",
    "# fico_apply_fix = {'850': '850-850'}\n",
    "# pmt_hist['fico_apply'] = pmt_hist['fico_apply'].replace(fico_apply_fix)\n",
    "# pmt_hist['fico_apply'] = (pmt_hist['fico_apply'].str[:3].astype(int) +\n",
    "#                           pmt_hist['fico_apply'].str[4:].astype(int)) / 2\n",
    "# pmt_hist['fico_apply'] = pmt_hist['fico_apply'].astype(int)\n",
    "\n",
    "# # fico_last ___________________________________________________________________\n",
    "# fico_last_fix = {'845-HIGH': '845-849', 'LOW-499': '495-499'}\n",
    "# pmt_hist['fico_last'] = pmt_hist['fico_last'].replace(fico_last_fix)\n",
    "# pmt_hist.ix[pmt_hist['fico_last'] != 'MISSING', 'fico_last'] = (\n",
    "#     pmt_hist.ix[pmt_hist['fico_last'] != 'MISSING', 'fico_last'].str[:3]\n",
    "#     .astype(int) + pmt_hist.ix[pmt_hist['fico_last'] != 'MISSING',\n",
    "#                                'fico_last'].str[4:].astype(int)) / 2\n",
    "# pmt_hist.ix[pmt_hist['fico_last'] == 'MISSING', 'fico_last'] = pmt_hist.ix[\n",
    "#     pmt_hist['fico_last'] == 'MISSING', 'fico_apply']\n",
    "# pmt_hist['fico_last'] = pmt_hist['fico_last'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # revol_credit_bal ____________________________________________________________\n",
    "# pmt_hist['revol_credit_bal'] = pmt_hist['revol_credit_bal'].astype(\n",
    "#     float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:34:26.003704Z",
     "start_time": "2018-02-19T04:34:25.982351Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # There is a problem with the inquiries 6m column. Some are nan values and some\n",
    "# # are marked '*' with no explanation. inq6m should be in loan info so dropping\n",
    "# pmt_hist.drop('inq_6m', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T04:35:33.887382Z",
     "start_time": "2018-02-19T04:35:33.860527Z"
    }
   },
   "outputs": [],
   "source": [
    "# # store pmt_history in hdfstore _______________________________________________\n",
    "# store = pd.HDFStore(\n",
    "#     dc.home_path+'/justin_tinkering/data_science/lendingclub/{0}_store.h5'.\n",
    "#     format(platform),\n",
    "#     append=True)\n",
    "\n",
    "# # Create min_itemsize_dict for allocating size when storing ___________________\n",
    "# min_itemsize_dict = {}\n",
    "# for col in pmt_hist.columns:\n",
    "#     if pmt_hist[col].dtype == np.object:\n",
    "#         print(col, pmt_hist[col].str.len().max())\n",
    "#         if col in ['State', 'VINTAGE', 'grade']:\n",
    "#             pass\n",
    "#         else:\n",
    "#             min_itemsize_dict[col] = 15\n",
    "\n",
    "# total_len = len(pmt_hist)\n",
    "# chunk_size = 120000 # 120k rows at a time\n",
    "# chunks = np.ceil(total_len/chunk_size)\n",
    "# df_chunks = np.array_split(pmt_hist, chunks)\n",
    "\n",
    "     \n",
    "# k = 0\n",
    "# for chunk in tqdm_notebook(df_chunks):\n",
    "#     if k == 0:\n",
    "#         store.append(\n",
    "#             'pmt_hist_intermediary_1',\n",
    "#             chunk,\n",
    "#             data_columns=True,\n",
    "#             index=True,\n",
    "#             append=False,\n",
    "#             min_itemsize=min_itemsize_dict)\n",
    "#         k += 1\n",
    "#     else:\n",
    "#         store.append(\n",
    "#             'pmt_hist_intermediary_1',\n",
    "#             chunk,\n",
    "#             data_columns=True,\n",
    "#             index=True,\n",
    "#             append=True)            \n",
    "        \n",
    "# # store pmt_hist ids        \n",
    "# pmt_hist_ids = pd.Series(pmt_hist['loan_id'].unique())\n",
    "# pmt_hist_ids.to_hdf(store, 'pmt_hist_ids', mode='w')        \n",
    "# print(store.keys())\n",
    "# print(store)        \n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
