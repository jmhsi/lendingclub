{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Justin Hsi\n",
    "## Part 2 of cleaning lending club payment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:48:54.488428Z",
     "start_time": "2018-08-25T08:48:54.459508Z"
    }
   },
   "outputs": [],
   "source": [
    "import dir_constants as dc\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "project = 'lendingclub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:48:54.521958Z",
     "start_time": "2018-08-25T08:48:54.490009Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_dupe_dates(group):\n",
    "    return pd.to_datetime(group[group.duplicated('date')]['date'].values)\n",
    "\n",
    "def merge_dupe_dates(group):\n",
    "    df_chunks = []\n",
    "    \n",
    "    dupe_dates = find_dupe_dates(group)\n",
    "    df_chunks.append(group[~group['date'].isin(dupe_dates)])\n",
    "    \n",
    "    for date in dupe_dates:\n",
    "        problem_rows = group[group['date'] == date]\n",
    "        ori_index = problem_rows.index\n",
    "        keep_row = problem_rows.iloc[-1].to_dict()\n",
    "        keep_row['outs_princp_beg'] = problem_rows.ix[ori_index[0],column_iloc_map['outs_princp_beg']]\n",
    "        \n",
    "        summed = problem_rows.sum()\n",
    "        keep_row['princp_paid'] = summed['princp_paid']\n",
    "        keep_row['int_paid'] = summed['int_paid']\n",
    "        keep_row['fee_paid'] = summed['fee_paid']\n",
    "        keep_row['amt_due'] = summed['amt_due']\n",
    "        keep_row['amt_paid'] = summed['amt_paid']\n",
    "        keep_row['charged_off_amt'] = summed['charged_off_amt']\n",
    "        keep_row['recovs'] = summed['recovs']\n",
    "        keep_row['recov_fees'] = summed['recov_fees']\n",
    "            \n",
    "        to_append = pd.DataFrame(keep_row, index=[ori_index[-1]])\n",
    "        df_chunks.append(to_append)\n",
    "    return pd.concat(df_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:49:13.275709Z",
     "start_time": "2018-08-25T08:48:54.523500Z"
    }
   },
   "outputs": [],
   "source": [
    "# !ls {dc.data_path+project}\n",
    "path = dc.data_path+project\n",
    "\n",
    "pmt_hist = pd.read_feather(path+'/pmt_hist_c1.fth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are loans that have multiple row entries per month (as in multiple pmts in same month) and there are also loans that don't have any entry for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:49:13.299319Z",
     "start_time": "2018-08-25T08:49:13.280423Z"
    }
   },
   "outputs": [],
   "source": [
    "column_iloc_map = {\n",
    "    col_name: pmt_hist.iloc[-1].index.get_loc(col_name)\n",
    "    for col_name in pmt_hist.columns.values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:49:29.266633Z",
     "start_time": "2018-08-25T08:49:13.302252Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_date_ids = pmt_hist[pmt_hist.duplicated(\n",
    "    ['loan_id', 'date'])]['loan_id'].unique()\n",
    "already_good = pmt_hist[~pmt_hist['loan_id'].isin(dup_date_ids)]\n",
    "needs_fixing = pmt_hist[pmt_hist['loan_id'].isin(dup_date_ids)]\n",
    "del pmt_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:52:24.017530Z",
     "start_time": "2018-08-25T08:49:29.268211Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10503 [00:00<?, ?it/s]/home/justin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "100%|██████████| 10503/10503 [02:54<00:00, 60.14it/s]\n"
     ]
    }
   ],
   "source": [
    "fixed_dfs = []\n",
    "id_grouped = needs_fixing.groupby('loan_id')\n",
    "for ids, group in tqdm(id_grouped):\n",
    "    if ids in dup_date_ids:\n",
    "        fixed_dfs.append(merge_dupe_dates(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:53:01.938509Z",
     "start_time": "2018-08-25T08:52:39.267180Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_df = pd.concat(fixed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:53:15.123070Z",
     "start_time": "2018-08-25T08:53:01.951587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmt_hist = pd.concat([already_good, fixed_df])\n",
    "del already_good, fixed_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:53:15.136697Z",
     "start_time": "2018-08-25T08:53:15.124641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37316502, 41)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmt_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store before next cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:53:15.165259Z",
     "start_time": "2018-08-25T08:53:15.139672Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T08:53:39.232531Z",
     "start_time": "2018-08-25T08:53:15.166610Z"
    }
   },
   "outputs": [],
   "source": [
    "pmt_hist.to_feather(path+'/pmt_hist_c2.fth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T06:09:13.632443Z",
     "start_time": "2018-08-25T06:09:04.339Z"
    }
   },
   "outputs": [],
   "source": [
    "# old version for chunking through hdfs\n",
    "# pmt_hist_ids = store['pmt_hist_ids'].astype(int)\n",
    "# max_id = pmt_hist_ids.max()\n",
    "# chunksize = 800\n",
    "# n_chunks = len(pmt_hist_ids)//chunksize + 1\n",
    "\n",
    "# # fix loans with double month entries _________________________________________\n",
    "# # left_bound = 0\n",
    "# # right_bound = pmt_hist_ids[chunksize]\n",
    "# already_good_dfs = []\n",
    "# fixed_dfs = []\n",
    "# k = 0\n",
    "# for n in tqdm_notebook(np.arange(n_chunks)):\n",
    "#     if n == 0:\n",
    "#         left_bound = 0\n",
    "#     else:\n",
    "#         left_bound = pmt_hist_ids[n*chunksize]\n",
    "#     if n == (n_chunks - 1):\n",
    "#         right_bound = max_id\n",
    "#     else:\n",
    "#         right_bound = pmt_hist_ids[(n+1)*chunksize]\n",
    "    \n",
    "#     chunk = pd.read_hdf(\n",
    "#         store,\n",
    "#         'pmt_hist_intermediary_1',\n",
    "#         where='(loan_id_num > left_bound) & (loan_id_num <= right_bound)')\n",
    "#     loans_with_two_entries_in_same_month = chunk[chunk.duplicated(\n",
    "#     ['loan_id', 'date'])]\n",
    "#     dup_date_ids = loans_with_two_entries_in_same_month['loan_id'].unique()\n",
    "#     if k == 0:\n",
    "#         column_iloc_map = {\n",
    "#             col_name: chunk.iloc[-1].index.get_loc(col_name)\n",
    "#             for col_name in chunk.columns.values\n",
    "#         }\n",
    "#         k += 1\n",
    "\n",
    "#     id_grouped = chunk.groupby('loan_id')\n",
    "#     already_good = chunk[~chunk['loan_id'].isin(dup_date_ids)]\n",
    "#     for ids, group in id_grouped:\n",
    "#         if ids in dup_date_ids:\n",
    "#             fixed_dfs.append(merge_dupe_dates(group))\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     already_good_dfs.append(already_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create min_itemsize_dict for allocating size when storing ___________________\n",
    "# min_itemsize_dict = {}\n",
    "# for col in already_good.columns:\n",
    "#     if already_good[col].dtype == np.object:\n",
    "#         print(col, already_good[col].str.len().max())\n",
    "#         if col in ['State', 'VINTAGE', 'grade']:\n",
    "#             pass\n",
    "#         else:\n",
    "#             min_itemsize_dict[col] = 15\n",
    "\n",
    "# col_dtype_map = already_good_dfs[0].dtypes.to_dict()\n",
    "# all_fixed_dfs = pd.concat(fixed_dfs)\n",
    "# for col, dtype in col_dtype_map.items():\n",
    "#     all_fixed_dfs[col] = all_fixed_dfs[col].astype(dtype)\n",
    "     \n",
    "# k = 0\n",
    "# for chunk in tqdm_notebook([all_fixed_dfs] + already_good_dfs):\n",
    "#     if k == 0:\n",
    "#         store.append(\n",
    "#             'pmt_hist_intermediary_2',\n",
    "#             chunk,\n",
    "#             data_columns=True,\n",
    "#             index=True,\n",
    "#             append=False,\n",
    "#             min_itemsize=min_itemsize_dict)\n",
    "#         k += 1\n",
    "#     else:\n",
    "#         store.append(\n",
    "#             'pmt_hist_intermediary_2',\n",
    "#             chunk,\n",
    "#             data_columns=True,\n",
    "#             index=True,\n",
    "#             append=True)           \n",
    "        \n",
    "# store.close()        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
