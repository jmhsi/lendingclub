{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science.lendingclub.dataprep_and_modeling.modeling_utils.data_prep_new as data_prep\n",
    "import dir_constants as dc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT FORGET TO DROP ISSUE_D AFTER PREPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'lendingclub'\n",
    "\n",
    "store = pd.HDFStore(\n",
    "    dc.home_path+'/justin_tinkering/data_science/lendingclub/{0}_store.h5'.\n",
    "    format(platform),\n",
    "    append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_info = store['train_filtered_columns']\n",
    "store.close()\n",
    "columns = loan_info.columns.values\n",
    "# checking dtypes to see which columns need one hotting, and which need null or not\n",
    "to_one_hot = []\n",
    "to_null_or_not = []\n",
    "do_nothing = []\n",
    "for col in columns:\n",
    "    if loan_info[col].dtypes == np.dtype('O'):\n",
    "#         print(col, loan_info[col].isnull().value_counts(dropna=False).to_dict())\n",
    "        to_one_hot.append(col)\n",
    "    elif len(loan_info[col].isnull().value_counts(dropna=False)) > 1:\n",
    "#         print(col, loan_info[col].isnull().value_counts(dropna=False).to_dict())\n",
    "        to_null_or_not.append(col)\n",
    "    else:\n",
    "#         print(col, loan_info[col].isnull().value_counts(dropna=False).to_dict())\n",
    "        do_nothing.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Until I figure out a good imputation method (e.g. bayes PCA), just drop columns with null still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized, eval_cols, mean_series, std_dev_series = data_prep.process_data_train(\n",
    "    loan_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in standardized.columns:\n",
    "#     if len(standardized[standardized[col].isnull()]) > 0:\n",
    "#         print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random search RF hyperparams, and build with best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_dict = {'n_estimators': np.arange(1,201),\n",
    "                   'criterion': ['mse'],#, 'mae'\n",
    "                   'max_features': [3,10, 50, 70, 100, 150, 200],\n",
    "                   'min_samples_split': [20, 200, 2000],\n",
    "                   'min_samples_leaf': [10, 100, 1000],\n",
    "                   'bootstrap': [True],\n",
    "                   'oob_score': [True],\n",
    "                   'n_jobs': [-1],\n",
    "                   'verbose': [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "random_search = RandomizedSearchCV(\n",
    "    regr, param_distributions=fit_params_dict, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "random_search.fit(standardized, eval_cols)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_)\n",
    "# Model with rank: 1\n",
    "# Mean validation score: 0.078 (std: 0.001)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 175, 'min_samples_split': 200, 'min_samples_leaf': 10, 'max_features': 70, 'criterion': 'mse', 'bootstrap': True}\n",
    "\n",
    "# Model with rank: 2\n",
    "# Mean validation score: 0.078 (std: 0.001)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 185, 'min_samples_split': 200, 'min_samples_leaf': 10, 'max_features': 100, 'criterion': 'mse', 'bootstrap': True}\n",
    "\n",
    "# Model with rank: 3\n",
    "# Mean validation score: 0.077 (std: 0.002)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 139, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 100, 'criterion': 'mse', 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take best hyperparams from report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(\n",
    "    n_estimators=175,\n",
    "    random_state=0,\n",
    "    max_features=70,\n",
    "    min_samples_split=200,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True,\n",
    "    bootstrap=True,\n",
    "    criterion='mse',\n",
    ")\n",
    "regr.fit(standardized, eval_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.score(standardized, eval_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the model\n",
    "joblib.dump(regr, 'model_dump/model_0.2.1.pkl')\n",
    "joblib.dump((mean_series, std_dev_series), 'model_dump/mean_stddev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y_%m_%d_%Hh_%Mm_%Ss\")\n",
    "# info to stick in detailed dataframe describing each model\n",
    "model_info = {'model_version': '0.2.1',\n",
    "              'target': 'npv_roi_10',\n",
    "              'weights': 'None',\n",
    "              'algo_model': 'RF_regr',\n",
    "              'hyperparams': \"bootstrap=True, criterion='mse', max_depth=None, max_features=70, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=10, min_samples_split=200, min_weight_fraction_leaf=0.0, n_estimators=175, n_jobs=-1, oob_score=True, random_state=0, verbose=0, warm_start=False\",\n",
    "              'cost_func': 'sklearn default, which I think is mse',\n",
    "              'useful_notes': 'R2 score of .3830649 (regr.score())',\n",
    "              'date': now}\n",
    "\n",
    "model_info_df = pd.DataFrame(model_info, index = ['0.2.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "model_info = store['model_info']\n",
    "model_info.ix['0.2.1',:] = model_info_df.values\n",
    "model_info.sort_index(inplace=True)\n",
    "store.append(\n",
    "            'model_info',\n",
    "            model_info_df,\n",
    "            data_columns=True,\n",
    "            index=True,\n",
    "            append=False,\n",
    ")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "test = store['test_filtered_columns']\n",
    "train = store['train_filtered_columns']\n",
    "loan_npv_rois = store['loan_npv_rois']\n",
    "default_series = test['target_strict']\n",
    "results = store['results']\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = data_prep.process_data_test(train)\n",
    "train_y = train_y['npv_roi_10'].values\n",
    "test_X, test_y = data_prep.process_data_test(test)\n",
    "test_y = test_y['npv_roi_10'].values\n",
    "regr = joblib.load('model_dump/model_0.2.1.pkl')\n",
    "regr_version = '0.2.1'\n",
    "test_yhat = regr.predict(test_X)\n",
    "train_yhat = regr.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = np.sum((test_yhat - test_y)**2)/len(test_y)\n",
    "train_mse = np.sum((train_yhat - train_y)**2)/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(trials, port_size, available_loans, regr, regr_version, test, loan_npv_rois,\n",
    "                default_series):\n",
    "    results = {}\n",
    "    pct_default = {}\n",
    "    test_copy = test.copy()\n",
    "    for trial in tqdm_notebook(np.arange(trials)):\n",
    "        loan_ids = np.random.choice(\n",
    "            test_copy.index.values, available_loans, replace=False)\n",
    "        loans_to_pick_from = test_copy.loc[loan_ids, :]\n",
    "        scores = regr.predict(loans_to_pick_from)\n",
    "        scores_series = pd.Series(dict(zip(loan_ids, scores)))\n",
    "        scores_series.sort_values(ascending=False, inplace=True)\n",
    "        picks = scores_series[:900].index.values\n",
    "        results[trial] = loan_npv_rois.loc[picks, :].mean().to_dict()\n",
    "        pct_default[trial] = (default_series.loc[picks].sum()) / port_size\n",
    "    pct_default_series = pd.Series(pct_default)\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df['pct_def'] = pct_default_series\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per done with baseline models, say 3000 loans available\n",
    "# , pick 900 of them\n",
    "trials = 20000\n",
    "port_size = 900\n",
    "available_loans = 3000\n",
    "model_results = eval_models(trials, port_size, available_loans, regr, regr_version, test_X, loan_npv_rois, default_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = []\n",
    "for col in model_results.columns.values:\n",
    "    multi_index.append((str(col),regr_version))\n",
    "append_results = model_results.copy()\n",
    "append_results.columns = pd.MultiIndex.from_tuples(multi_index, names = ['discount_rate', 'model'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index_results = []\n",
    "for col in results.columns.values:\n",
    "    multi_index_results.append((str(col[0]), col[1]))\n",
    "results.columns = pd.MultiIndex.from_tuples(multi_index_results, names = ['discount_rate', 'model'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    results = results.join(append_results)\n",
    "except ValueError:\n",
    "    results.loc[:, (slice(None), slice('0.2.1','0.2.1'))] = append_results\n",
    "results.sort_index(axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "store['results'] = full_results\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
