{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T12:30:37.749504Z",
     "start_time": "2018-03-11T12:30:37.097356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%writefile` not found (But cell magic `%%writefile` exists, did you mean that instead?).\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.imports'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6272ec58e7c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_imports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.imports'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "# %load \n",
    "/home/justin/justin_tinkering/data_science/lendingclub/dataprep_and_modeling/fastai/learner.py\n",
    "from .imports import *\n",
    "from .torch_imports import *\n",
    "from .core import *\n",
    "from .transforms import *\n",
    "from .model import *\n",
    "from .dataset import *\n",
    "from .sgdr import *\n",
    "from .layer_optimizer import *\n",
    "from .layers import *\n",
    "from .metrics import *\n",
    "from .losses import *\n",
    "import time\n",
    "\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, data, models, opt_fn=None, tmp_name='tmp', models_name='models', metrics=None, clip=None):\n",
    "        self.data_,self.models,self.metrics = data,models,metrics\n",
    "        self.sched=None\n",
    "        self.wd_sched = None\n",
    "        self.clip = None\n",
    "        self.opt_fn = opt_fn or SGD_Momentum(0.9)\n",
    "        self.tmp_path = os.path.join(self.data.path, tmp_name)\n",
    "        self.models_path = os.path.join(self.data.path, models_name)\n",
    "        os.makedirs(self.tmp_path, exist_ok=True)\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        self.crit,self.reg_fn = None,None\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_data(cls, m, data, **kwargs):\n",
    "        self = cls(data, BasicModel(to_gpu(m)), **kwargs)\n",
    "        self.unfreeze()\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self,i): return self.children[i]\n",
    "\n",
    "    @property\n",
    "    def children(self): return children(self.model)\n",
    "\n",
    "    @property\n",
    "    def model(self): return self.models.model\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.data_\n",
    "\n",
    "    def summary(self): return model_summary(self.model, [3,self.data.sz,self.data.sz])\n",
    "\n",
    "    def __repr__(self): return self.model.__repr__()\n",
    "\n",
    "    def set_bn_freeze(self, m, do_freeze):\n",
    "        if hasattr(m, 'running_mean'): m.bn_freeze = do_freeze\n",
    "\n",
    "    def bn_freeze(self, do_freeze):\n",
    "        apply_leaf(self.model, lambda m: self.set_bn_freeze(m, do_freeze))\n",
    "\n",
    "    def freeze_to(self, n):\n",
    "        c=self.get_layer_groups()\n",
    "        for l in c:     set_trainable(l, False)\n",
    "        for l in c[n:]: set_trainable(l, True)\n",
    "\n",
    "    def unfreeze(self): self.freeze_to(0)\n",
    "\n",
    "    def get_model_path(self, name): return os.path.join(self.models_path,name)+'.h5'\n",
    "    def save(self, name): save_model(self.model, self.get_model_path(name))\n",
    "    def load(self, name): load_model(self.model, self.get_model_path(name))\n",
    "\n",
    "    def set_data(self, data): self.data_ = data\n",
    "\n",
    "    def get_cycle_end(self, name):\n",
    "        if name is None: return None\n",
    "        return lambda sched, cycle: self.save_cycle(name, cycle)\n",
    "\n",
    "    def save_cycle(self, name, cycle): self.save(f'{name}_cyc_{cycle}')\n",
    "    def load_cycle(self, name, cycle): self.load(f'{name}_cyc_{cycle}')\n",
    "\n",
    "    def fit_gen(self, model, data, layer_opt, n_cycle, cycle_len=None, cycle_mult=1, cycle_save_name=None, best_save_name=None,\n",
    "                use_clr=None, metrics=None, callbacks=None, use_wd_sched=False, norm_wds=False, wds_sched_mult=None, **kwargs):\n",
    "\n",
    "        \"\"\"Method does some preparation before finally delegating to the 'fit' method for\n",
    "        fitting the model. Namely, if cycle_len is defined, it adds a 'Cosine Annealing'\n",
    "        scheduler for varying the learning rate across iterations.\n",
    "\n",
    "        Method also computes the total number of epochs to fit based on provided 'cycle_len',\n",
    "        'cycle_mult', and 'n_cycle' parameters.\n",
    "\n",
    "        Args:\n",
    "            model (Learner):  Any neural architecture for solving a supported problem.\n",
    "                Eg. ResNet-34, RNN_Learner etc.\n",
    "\n",
    "            data (ModelData): An instance of ModelData.\n",
    "\n",
    "            layer_opt (LayerOptimizer): An instance of the LayerOptimizer class\n",
    "\n",
    "            n_cycle (int): number of cycles\n",
    "\n",
    "            cycle_len (int):  number of cycles before lr is reset to the initial value.\n",
    "                E.g if cycle_len = 3, then the lr is varied between a maximum\n",
    "                and minimum value over 3 epochs.\n",
    "\n",
    "            cycle_mult (int): additional parameter for influencing how the lr resets over\n",
    "                the cycles. For an intuitive explanation, please see\n",
    "                https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb\n",
    "\n",
    "            cycle_save_name (str): use to save the weights at end of each cycle\n",
    "            \n",
    "            best_save_name (str): use to save weights of best model during training.\n",
    "\n",
    "            metrics (function): some function for evaluating a desired metric. Eg. accuracy.\n",
    "\n",
    "            callbacks (list(Callback)): callbacks to apply during the training.\n",
    "\n",
    "            use_wd_sched (bool, optional): set to True to enable weight regularization using\n",
    "                the technique mentioned in https://arxiv.org/abs/1711.05101. When this is True\n",
    "                alone (see below), the regularization is detached from gradient update and\n",
    "                applied directly to the weights.\n",
    "\n",
    "            norm_wds (bool, optional): when this is set to True along with use_wd_sched, the\n",
    "                regularization factor is normalized with each training cycle.\n",
    "\n",
    "            wds_sched_mult (function, optional): when this is provided along with use_wd_sched\n",
    "                as True, the value computed by this function is multiplied with the regularization\n",
    "                strength. This function is passed the WeightDecaySchedule object. And example\n",
    "                function that can be passed is:\n",
    "                            f = lambda x: np.array(x.layer_opt.lrs) / x.init_lrs\n",
    "\n",
    "            kwargs: other optional arguments\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if callbacks is None: callbacks=[]\n",
    "        if metrics is None: metrics=self.metrics\n",
    "\n",
    "        if use_wd_sched:\n",
    "            # This needs to come before CosAnneal() because we need to read the initial learning rate from\n",
    "            # layer_opt.lrs - but CosAnneal() alters the layer_opt.lrs value initially (divides by 100)\n",
    "            if np.sum(layer_opt.wds) == 0:\n",
    "                print('fit() warning: use_wd_sched is set to True, but weight decay(s) passed are 0. Use wds to '\n",
    "                      'pass weight decay values.')\n",
    "            batch_per_epoch = len(data.trn_dl)\n",
    "            cl = cycle_len if cycle_len else 1\n",
    "            self.wd_sched = WeightDecaySchedule(layer_opt, batch_per_epoch, cl, cycle_mult, n_cycle,\n",
    "                                                norm_wds, wds_sched_mult)\n",
    "            callbacks += [self.wd_sched]\n",
    "\n",
    "        elif use_clr is not None:\n",
    "            clr_div,cut_div = use_clr\n",
    "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
    "            self.sched = CircularLR(layer_opt, len(data.trn_dl)*cycle_len, on_cycle_end=cycle_end, div=clr_div, cut_div=cut_div)\n",
    "        elif cycle_len:\n",
    "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
    "            cycle_batches = len(data.trn_dl)*cycle_len\n",
    "            self.sched = CosAnneal(layer_opt, cycle_batches, on_cycle_end=cycle_end, cycle_mult=cycle_mult)\n",
    "        elif not self.sched: self.sched=LossRecorder(layer_opt)\n",
    "        callbacks+=[self.sched]\n",
    "        \n",
    "        if best_save_name is not None:\n",
    "            callbacks+=[SaveBestModel(self, layer_opt, best_save_name)]\n",
    "            \n",
    "        n_epoch = sum_geom(cycle_len if cycle_len else 1, cycle_mult, n_cycle)\n",
    "        return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n",
    "            metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n",
    "\n",
    "    def get_layer_groups(self): return self.models.get_layer_groups()\n",
    "\n",
    "    def get_layer_opt(self, lrs, wds):\n",
    "\n",
    "        \"\"\"Method returns an instance of the LayerOptimizer class, which\n",
    "        allows for setting differential learning rates for different\n",
    "        parts of the model.\n",
    "\n",
    "        An example of how a model maybe differentiated into different parts\n",
    "        for application of differential learning rates and weight decays is\n",
    "        seen in ../.../courses/dl1/fastai/conv_learner.py, using the dict\n",
    "        'model_meta'. Currently, this seems supported only for convolutional\n",
    "        networks such as VGG-19, ResNet-XX etc.\n",
    "\n",
    "        Args:\n",
    "            lrs (float or list(float)): learning rate(s) for the model\n",
    "\n",
    "            wds (float or list(float)): weight decay parameter(s).\n",
    "\n",
    "        Returns:\n",
    "            An instance of a LayerOptimizer\n",
    "        \"\"\"\n",
    "        return LayerOptimizer(self.opt_fn, self.get_layer_groups(), lrs, wds)\n",
    "\n",
    "    def fit(self, lrs, n_cycle, wds=None, **kwargs):\n",
    "\n",
    "        \"\"\"Method gets an instance of LayerOptimizer and delegates to self.fit_gen(..)\n",
    "\n",
    "        Note that one can specify a list of learning rates which, when appropriately\n",
    "        defined, will be applied to different segments of an architecture. This seems\n",
    "        mostly relevant to ImageNet-trained models, where we want to alter the layers\n",
    "        closest to the images by much smaller amounts.\n",
    "\n",
    "        Likewise, a single or list of weight decay parameters can be specified, which\n",
    "        if appropriate for a model, will apply variable weight decay parameters to\n",
    "        different segments of the model.\n",
    "\n",
    "        Args:\n",
    "            lrs (float or list(float)): learning rate for the model\n",
    "\n",
    "            n_cycle (int): number of cycles (or iterations) to fit the model for\n",
    "\n",
    "            wds (float or list(float)): weight decay parameter(s).\n",
    "\n",
    "            kwargs: other arguments\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.sched = None\n",
    "        layer_opt = self.get_layer_opt(lrs, wds)\n",
    "        return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
    "\n",
    "    def warm_up(self, lr, wds=None):\n",
    "        layer_opt = self.get_layer_opt(lr/4, wds)\n",
    "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), lr, linear=True)\n",
    "        return self.fit_gen(self.model, self.data, layer_opt, 1)\n",
    "\n",
    "    def lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False):\n",
    "        \"\"\"Helps you find an optimal learning rate for a model.\n",
    "\n",
    "         It uses the technique developed in the 2015 paper\n",
    "         `Cyclical Learning Rates for Training Neural Networks`, where\n",
    "         we simply keep increasing the learning rate from a very small value,\n",
    "         until the loss starts decreasing.\n",
    "\n",
    "        Args:\n",
    "            start_lr (float/numpy array) : Passing in a numpy array allows you\n",
    "                to specify learning rates for a learner's layer_groups\n",
    "            end_lr (float) : The maximum learning rate to try.\n",
    "            wds (iterable/float)\n",
    "\n",
    "        Examples:\n",
    "            As training moves us closer to the optimal weights for a model,\n",
    "            the optimal learning rate will be smaller. We can take advantage of\n",
    "            that knowledge and provide lr_find() with a starting learning rate\n",
    "            1000x smaller than the model's current learning rate as such:\n",
    "\n",
    "            >> learn.lr_find(lr/1000)\n",
    "\n",
    "            >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\n",
    "            >> learn.lr_find(lrs / 1000)\n",
    "\n",
    "        Notes:\n",
    "            lr_find() may finish before going through each batch of examples if\n",
    "            the loss decreases enough.\n",
    "\n",
    "        .. _Cyclical Learning Rates for Training Neural Networks:\n",
    "            http://arxiv.org/abs/1506.01186\n",
    "\n",
    "        \"\"\"\n",
    "        self.save('tmp')\n",
    "        layer_opt = self.get_layer_opt(start_lr, wds)\n",
    "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n",
    "        self.fit_gen(self.model, self.data, layer_opt, 1)\n",
    "        self.load('tmp')\n",
    "\n",
    "    def predict(self, is_test=False):\n",
    "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
    "        return predict(self.model, dl)\n",
    "\n",
    "    def predict_with_targs(self, is_test=False):\n",
    "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
    "        return predict_with_targs(self.model, dl)\n",
    "\n",
    "    def predict_dl(self, dl): return predict_with_targs(self.model, dl)[0]\n",
    "    def predict_array(self, arr): return to_np(self.model(V(T(arr).cuda())))\n",
    "\n",
    "    def TTA(self, n_aug=4, is_test=False):\n",
    "        \"\"\" Predict with Test Time Augmentation (TTA)\n",
    "\n",
    "        Additional to the original test/validation images, apply image augmentation to them\n",
    "        (just like for training images) and calculate the mean of predictions. The intent\n",
    "        is to increase the accuracy of predictions by examining the images using multiple\n",
    "        perspectives.\n",
    "\n",
    "        Args:\n",
    "            n_aug: a number of augmentation images to use per original image\n",
    "            is_test: indicate to use test images; otherwise use validation images\n",
    "\n",
    "        Returns:\n",
    "            (tuple): a tuple containing:\n",
    "\n",
    "                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\n",
    "                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.\n",
    "        \"\"\"\n",
    "        dl1 = self.data.test_dl     if is_test else self.data.val_dl\n",
    "        dl2 = self.data.test_aug_dl if is_test else self.data.aug_dl\n",
    "        preds1,targs = predict_with_targs(self.model, dl1)\n",
    "        preds1 = [preds1]*math.ceil(n_aug/4)\n",
    "        preds2 = [predict_with_targs(self.model, dl2)[0] for i in tqdm(range(n_aug), leave=False)]\n",
    "        return np.stack(preds1+preds2), targs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T20:41:46.600838Z",
     "start_time": "2018-03-11T20:41:46.579602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/justin/justin_tinkering/data_science/lendingclub/dataprep_and_modeling/fastai/column_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/justin_tinkering/data_science/lendingclub/dataprep_and_modeling/fastai/column_data.py\n",
    "from .imports import *\n",
    "from .torch_imports import *\n",
    "from .dataset import *\n",
    "from .learner import *\n",
    "\n",
    "\n",
    "class PassthruDataset(Dataset):\n",
    "    def __init__(self,*args):\n",
    "        *xs,y=args\n",
    "        self.xs,self.y = xs,y\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return [o[idx] for o in self.xs] + [self.y[idx]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(self, df, cols_x, col_y):\n",
    "        cols = [df[o] for o in cols_x+[col_y]]\n",
    "        return self(*cols)\n",
    "\n",
    "\n",
    "class ColumnarDataset(Dataset):\n",
    "    def __init__(self, cats, conts, y):\n",
    "        n = len(cats[0]) if cats else len(conts[0])\n",
    "        self.cats = np.stack(cats, 1).astype(np.int64) if cats else np.zeros((n,1))\n",
    "        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n",
    "        self.y = np.zeros((n,1)) if y is None else y[:,None]\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, df_cat, df_cont, y=None):\n",
    "        cat_cols = [c.values for n,c in df_cat.items()]\n",
    "        cont_cols = [c.values for n,c in df_cont.items()]\n",
    "        return cls(cat_cols, cont_cols, y)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, df, cat_flds, y=None):\n",
    "        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y)\n",
    "\n",
    "\n",
    "class ColumnarModelData(ModelData):\n",
    "    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None, shuffle=True):\n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n",
    "        super().__init__(path, DataLoader(trn_ds, bs, shuffle=shuffle, num_workers=1),\n",
    "            DataLoader(val_ds, bs*2, shuffle=False, num_workers=1), test_dl)\n",
    "\n",
    "    @classmethod\n",
    "    def from_arrays(cls, path, val_idxs, xs, y, bs=64, test_xs=None, shuffle=True):\n",
    "        ((val_xs, trn_xs), (val_y, trn_y)) = split_by_idx(val_idxs, xs, y)\n",
    "        test_ds = PassthruDataset(*(test_xs.T), [0] * len(test_xs)) if test_xs is not None else None\n",
    "        return cls(path, PassthruDataset(*(trn_xs.T), trn_y), PassthruDataset(*(val_xs.T), val_y),\n",
    "                   bs=bs, shuffle=shuffle, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df=None):\n",
    "        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds) if test_df is not None else None\n",
    "        return cls(path, ColumnarDataset.from_data_frame(trn_df, cat_flds, trn_y),\n",
    "                    ColumnarDataset.from_data_frame(val_df, cat_flds, val_y), bs, test_ds=test_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, val_idxs, df, y, cat_flds, bs, test_df=None):\n",
    "        ((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idxs, df, y)\n",
    "        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, test_df=test_df)\n",
    "\n",
    "    def get_learner(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                    y_range=None, use_bn=False, **kwargs):\n",
    "        model = MixedInputModel(emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn)\n",
    "        return StructuredLearner(self, StructuredModel(to_gpu(model)), opt_fn=optim.Adam, **kwargs)\n",
    "\n",
    "\n",
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)\n",
    "\n",
    "\n",
    "class MixedInputModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                 y_range=None, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: kaiming_normal(o.weight.data)\n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        for l,d,b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x))\n",
    "            if self.use_bn: x = b(x)\n",
    "            x = d(x)\n",
    "        x = self.outp(x)\n",
    "        if self.y_range:\n",
    "            x = F.sigmoid(x)\n",
    "            x = x*(self.y_range[1] - self.y_range[0])\n",
    "            x = x+self.y_range[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class StructuredLearner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        self.crit = F.mse_loss\n",
    "\n",
    "class StructuredModel(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        m=self.model\n",
    "        return [m.embs, children(m.lins)+children(m.bns), m.outp]\n",
    "\n",
    "\n",
    "class CollabFilterDataset(Dataset):\n",
    "    def __init__(self, path, user_col, item_col, ratings):\n",
    "        self.ratings,self.path = ratings.values.astype(np.float32),path\n",
    "        self.n = len(ratings)\n",
    "        (self.users,self.user2idx,self.user_col,self.n_users) = self.proc_col(user_col)\n",
    "        (self.items,self.item2idx,self.item_col,self.n_items) = self.proc_col(item_col)\n",
    "        self.min_score,self.max_score = min(ratings),max(ratings)\n",
    "        self.cols = [self.user_col,self.item_col,self.ratings]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, path, df, user_name, item_name, rating_name):\n",
    "        return cls(path, df[user_name], df[item_name], df[rating_name])\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, path, csv, user_name, item_name, rating_name):\n",
    "        df = pd.read_csv(os.path.join(path,csv))\n",
    "        return cls.from_data_frame(path, df, user_name, item_name, rating_name)\n",
    "\n",
    "    def proc_col(self,col):\n",
    "        uniq = col.unique()\n",
    "        name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "        return (uniq, name2idx, np.array([name2idx[x] for x in col]), len(uniq))\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "    def __getitem__(self, idx): return [o[idx] for o in self.cols]\n",
    "\n",
    "    def get_data(self, val_idxs, bs):\n",
    "        val, trn = zip(*split_by_idx(val_idxs, *self.cols))\n",
    "        return ColumnarModelData(self.path, PassthruDataset(*trn), PassthruDataset(*val), bs)\n",
    "\n",
    "    def get_model(self, n_factors):\n",
    "        model = EmbeddingDotBias(n_factors, self.n_users, self.n_items, self.min_score, self.max_score)\n",
    "        return CollabFilterModel(to_gpu(model))\n",
    "\n",
    "    def get_learner(self, n_factors, val_idxs, bs, **kwargs):\n",
    "        return CollabFilterLearner(self.get_data(val_idxs, bs), self.get_model(n_factors), **kwargs)\n",
    "\n",
    "\n",
    "def get_emb(ni,nf):\n",
    "    e = nn.Embedding(ni, nf)\n",
    "    e.weight.data.uniform_(-0.05,0.05)\n",
    "    return e\n",
    "\n",
    "class EmbeddingDotBias(nn.Module):\n",
    "    def __init__(self, n_factors, n_users, n_items, min_score, max_score):\n",
    "        super().__init__()\n",
    "        self.min_score,self.max_score = min_score,max_score\n",
    "        (self.u, self.i, self.ub, self.ib) = [get_emb(*o) for o in [\n",
    "            (n_users, n_factors), (n_items, n_factors), (n_users,1), (n_items,1)\n",
    "        ]]\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        um = self.u(users)* self.i(items)\n",
    "        res = um.sum(1) + self.ub(users).squeeze() + self.ib(items).squeeze()\n",
    "        return F.sigmoid(res) * (self.max_score-self.min_score) + self.min_score\n",
    "\n",
    "class CollabFilterLearner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        self.crit = F.mse_loss\n",
    "\n",
    "class CollabFilterModel(BasicModel):\n",
    "    def get_layer_groups(self): return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T07:52:50.522954Z",
     "start_time": "2018-03-12T07:52:50.505391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/justin/justin_tinkering/data_science/lendingclub/dataprep_and_modeling/fastai/core.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/justin_tinkering/data_science/lendingclub/dataprep_and_modeling/fastai/core.py\n",
    "# %load\n",
    "from .imports import *\n",
    "from .torch_imports import *\n",
    "\n",
    "def sum_geom(a,r,n): return a*n if r==1 else math.ceil(a*(1-r**n)/(1-r))\n",
    "\n",
    "conv_dict = {np.dtype('int8'): torch.LongTensor, np.dtype('int16'): torch.LongTensor,\n",
    "    np.dtype('int32'): torch.LongTensor, np.dtype('int64'): torch.LongTensor,\n",
    "    np.dtype('float32'): torch.FloatTensor, np.dtype('float64'): torch.FloatTensor}\n",
    "\n",
    "def T(a):\n",
    "    if torch.is_tensor(a): res = a\n",
    "    else:\n",
    "        a = np.array(np.ascontiguousarray(a))\n",
    "        if a.dtype in (np.int8, np.int16, np.int32, np.int64):\n",
    "            res = torch.LongTensor(a.astype(np.int64))\n",
    "        elif a.dtype in (np.float32, np.float64):\n",
    "            res = torch.FloatTensor(a.astype(np.float32))\n",
    "        else: raise NotImplementedError(a.dtype)\n",
    "    return to_gpu(res, async=True)\n",
    "\n",
    "def create_variable(x, volatile, requires_grad=False):\n",
    "    if not isinstance(x, Variable):\n",
    "        x = Variable(T(x), volatile=volatile, requires_grad=requires_grad)\n",
    "    return x\n",
    "\n",
    "def V_(x, requires_grad=False):\n",
    "    return create_variable(x, False, requires_grad=requires_grad)\n",
    "def V(x, requires_grad=False):\n",
    "    return [V_(o, requires_grad) for o in x] if isinstance(x,list) else V_(x, requires_grad)\n",
    "\n",
    "def VV_(x): return create_variable(x, True)\n",
    "def VV(x):  return [VV_(o) for o in x] if isinstance(x,list) else VV_(x)\n",
    "\n",
    "def to_np(v):\n",
    "    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\n",
    "    if isinstance(v, Variable): v=v.data\n",
    "    return v.cpu().numpy()\n",
    "\n",
    "USE_GPU=False\n",
    "def to_gpu(x, *args, **kwargs):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return x.cuda(*args, **kwargs) if torch.cuda.is_available() and USE_GPU else x\n",
    "\n",
    "def noop(*args, **kwargs): return\n",
    "\n",
    "def split_by_idxs(seq, idxs):\n",
    "    last = 0\n",
    "    for idx in idxs:\n",
    "        yield seq[last:idx]\n",
    "        last = idx\n",
    "    yield seq[last:]\n",
    "\n",
    "def trainable_params_(m):\n",
    "    return [p for p in m.parameters() if p.requires_grad]\n",
    "\n",
    "def chain_params(p):\n",
    "    if isinstance(p, (list,tuple)):\n",
    "        return list(chain(*[trainable_params_(o) for o in p]))\n",
    "    return trainable_params_(p)\n",
    "\n",
    "def set_trainable_attr(m,b):\n",
    "    m.trainable=b\n",
    "    for p in m.parameters(): p.requires_grad=b\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    if len(c)>0:\n",
    "        for l in c: apply_leaf(l,f)\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m,b))\n",
    "\n",
    "def SGD_Momentum(momentum):\n",
    "    return lambda *args, **kwargs: optim.SGD(*args, momentum=momentum, **kwargs)\n",
    "\n",
    "def one_hot(a,c): return np.eye(c)[a]\n",
    "\n",
    "def partition(a, sz): return [a[i:i+sz] for i in range(0, len(a), sz)]\n",
    "\n",
    "def partition_by_cores(a):\n",
    "    return partition(a, len(a)//num_cpus() + 1)\n",
    "\n",
    "def num_cpus():\n",
    "    try:\n",
    "        return len(os.sched_getaffinity(0))\n",
    "    except AttributeError:\n",
    "        return os.cpu_count()\n",
    "\n",
    "\n",
    "class BasicModel():\n",
    "    def __init__(self,model,name='unnamed'): self.model,self.name = model,name\n",
    "    def get_layer_groups(self, do_fc=False): return children(self.model)\n",
    "\n",
    "class SingleModel(BasicModel):\n",
    "    def get_layer_groups(self): return [self.model]\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return F.log_softmax(l_x, dim=-1)\n",
    "\n",
    "\n",
    "def save(fn, a): pickle.dump(a, open(fn,'wb'))\n",
    "def load(fn): return pickle.load(open(fn,'rb'))\n",
    "def load2(fn): return pickle.load(open(fn,'rb'), encoding='iso-8859-1')\n",
    "\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
