{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science.lendingclub.dataprep_and_modeling.modeling_utils.data_prep_new as data_prep\n",
    "import dir_constants as dc\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 100): \n",
    "        with pd.option_context(\"display.max_columns\", 1000): \n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT FORGET TO DROP ISSUE_D AFTER PREPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'lendingclub'\n",
    "\n",
    "store = pd.HDFStore(\n",
    "    dc.home_path+'/justin_tinkering/data_science/lendingclub/{0}_store.h5'.\n",
    "    format(platform),\n",
    "    append=True)\n",
    "loan_info = store['loan_info_clean']\n",
    "filtered_col_example = store['base_dataset_filtered_columns']\n",
    "base_cols = filtered_col_example.columns\n",
    "del filtered_col_example\n",
    "npv_rois = store['loan_npv_rois']\n",
    "loan_info['npv_roi_10'] = npv_rois[0.1]\n",
    "del npv_rois\n",
    "# loan_info = loan_info[base_cols]\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to split dataset into train and validation. Split validation in half for valid and test\n",
    "sbd = loan_info[loan_info['maturity_time'] >= .98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sbd['issue_d'].value_counts(dropna=False).index.max()))\n",
    "# last date is 2014-10-01\n",
    "# for 36 month, lets say 2014 july/aug/sep/oct are valid/test, everything before is train for normal loans\n",
    "# assume 4 month to charge off for defaulting loans, so july aug sep oct still valid/test, but\n",
    "# last pmt date for defaulting should be Feb.\n",
    "# for 60 month, same as above but 2012 dates\n",
    "\n",
    "    \n",
    "#     (sbd['last_pymnt_d'] <= '2014-06-01') & (sbd['loan_status'].isin(['paid', 'current']))) |((sbd['last_pymnt_d'] <= '2014-02-01') & (~sbd['loan_status'].isin(['paid', 'current'])))))\n",
    "\n",
    "train_mask_36 = ((sbd['term'] == 36) & ( ((sbd['last_pymnt_d'] <= '2014-06-01') & (sbd['loan_status'].isin(['paid', 'current'])))  | ((sbd['last_pymnt_d'] <= '2014-02-01') & (~sbd['loan_status'].isin(['paid', 'current']))) ))\n",
    "\n",
    "train_mask_60 = ((sbd['term'] == 60) & ( ((sbd['last_pymnt_d'] <= '2012-06-01') & (sbd['loan_status'].isin(['paid', 'current'])))  | ((sbd['last_pymnt_d'] <= '2012-02-01') & (~sbd['loan_status'].isin(['paid', 'current']))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.array(pd.concat([sbd[train_mask_36], sbd[train_mask_60]])['id'])\n",
    "# all_idx = np.array(sbd.index)\n",
    "test_idx = np.array(sbd[~sbd['id'].isin(train_idx)].index)\n",
    "assert (len(train_idx) + len(test_idx)) == len(sbd)\n",
    "valid_idx = np.random.choice(test_idx, len(test_idx)//2, replace=False)\n",
    "va_set = set(valid_idx)\n",
    "te_set = set(test_idx).difference(va_set)\n",
    "tr_set = set(train_idx)\n",
    "assert len(te_set.intersection(va_set)) == 0\n",
    "test_idx = np.array(list(te_set))\n",
    "assert (len(train_idx)+ len(test_idx)+ len(valid_idx)) == len(sbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd = sbd[base_cols]\n",
    "cols_to_drop = ['hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount',\n",
    "                'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_length',\n",
    "                'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest',\n",
    "                'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'maturity_time', 'maturity_paid',\n",
    "                'target_loose','installment_amount', 'npv_roi_10', 'orig_amt_due',]\n",
    "\n",
    "sbd.drop(cols_to_drop, axis=1, inplace=True)\n",
    "sbd.rename({'issue_d': 'issue_date'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(sbd, 'issue_date')\n",
    "train_cats(sbd)\n",
    "df, y, nas = proc_df(sbd, 'target_strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(y, index=df.index, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in temp\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "X_train = df.ix[train_idx,:].fillna(0).reset_index(drop=True).to_feather('tmp/X_train')\n",
    "X_valid = df.ix[valid_idx,:].fillna(0).reset_index(drop=True).to_feather('tmp/X_valid')\n",
    "X_test = df.ix[test_idx,:].fillna(0).reset_index(drop=True).to_feather('tmp/X_test')\n",
    "y_train = y.ix[train_idx].reset_index(drop=True).to_feather('tmp/y_train')\n",
    "y_valid = y.ix[valid_idx].reset_index(drop=True).to_feather('tmp/y_valid')\n",
    "y_test = y.ix[test_idx].reset_index(drop=True).to_feather('tmp/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "X_train = pd.read_feather('tmp/X_train')\n",
    "X_valid = pd.read_feather('tmp/X_valid')\n",
    "X_test = pd.read_feather('tmp/X_test')\n",
    "y_train = pd.read_feather('tmp/y_train')\n",
    "y_valid = pd.read_feather('tmp/y_valid')\n",
    "y_test = pd.read_feather('tmp/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestClassifier(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "m.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return mean_squared_error(x, y)\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestClassifier(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "103571/len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid['target'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.utcnow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change depth\n",
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random search RF hyperparams, and build with best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_dict = {'n_estimators': np.arange(1,201),\n",
    "                   'criterion': ['mse'],#, 'mae'\n",
    "                   'max_features': [3,10, 50, 70, 100, 150, 200],\n",
    "                   'min_samples_split': [20, 200, 2000],\n",
    "                   'min_samples_leaf': [10, 100, 1000],\n",
    "                   'bootstrap': [True],\n",
    "                   'oob_score': [True],\n",
    "                   'n_jobs': [-1],\n",
    "                   'verbose': [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "random_search = RandomizedSearchCV(\n",
    "    regr, param_distributions=fit_params_dict, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "random_search.fit(standardized, eval_cols)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_)\n",
    "# Model with rank: 1\n",
    "# Mean validation score: 0.078 (std: 0.001)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 175, 'min_samples_split': 200, 'min_samples_leaf': 10, 'max_features': 70, 'criterion': 'mse', 'bootstrap': True}\n",
    "\n",
    "# Model with rank: 2\n",
    "# Mean validation score: 0.078 (std: 0.001)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 185, 'min_samples_split': 200, 'min_samples_leaf': 10, 'max_features': 100, 'criterion': 'mse', 'bootstrap': True}\n",
    "\n",
    "# Model with rank: 3\n",
    "# Mean validation score: 0.077 (std: 0.002)\n",
    "# Parameters: {'verbose': 10, 'oob_score': True, 'n_jobs': -1, 'n_estimators': 139, 'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 100, 'criterion': 'mse', 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take best hyperparams from report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(\n",
    "    n_estimators=175,\n",
    "    random_state=0,\n",
    "    max_features=70,\n",
    "    min_samples_split=200,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True,\n",
    "    bootstrap=True,\n",
    "    criterion='mse',\n",
    ")\n",
    "regr.fit(standardized, eval_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.score(standardized, eval_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the model\n",
    "joblib.dump(regr, 'model_dump/model_0.2.1.pkl')\n",
    "joblib.dump((mean_series, std_dev_series), 'model_dump/mean_stddev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y_%m_%d_%Hh_%Mm_%Ss\")\n",
    "# info to stick in detailed dataframe describing each model\n",
    "model_info = {'model_version': '0.2.1',\n",
    "              'target': 'npv_roi_10',\n",
    "              'weights': 'None',\n",
    "              'algo_model': 'RF_regr',\n",
    "              'hyperparams': \"bootstrap=True, criterion='mse', max_depth=None, max_features=70, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=10, min_samples_split=200, min_weight_fraction_leaf=0.0, n_estimators=175, n_jobs=-1, oob_score=True, random_state=0, verbose=0, warm_start=False\",\n",
    "              'cost_func': 'sklearn default, which I think is mse',\n",
    "              'useful_notes': 'R2 score of .3830649 (regr.score())',\n",
    "              'date': now}\n",
    "\n",
    "model_info_df = pd.DataFrame(model_info, index = ['0.2.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "model_info = store['model_info']\n",
    "model_info.ix['0.2.1',:] = model_info_df.values\n",
    "model_info.sort_index(inplace=True)\n",
    "store.append(\n",
    "            'model_info',\n",
    "            model_info_df,\n",
    "            data_columns=True,\n",
    "            index=True,\n",
    "            append=False,\n",
    ")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "test = store['test_filtered_columns']\n",
    "train = store['train_filtered_columns']\n",
    "loan_npv_rois = store['loan_npv_rois']\n",
    "default_series = test['target_strict']\n",
    "results = store['results']\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = data_prep.process_data_test(train)\n",
    "train_y = train_y['npv_roi_10'].values\n",
    "test_X, test_y = data_prep.process_data_test(test)\n",
    "test_y = test_y['npv_roi_10'].values\n",
    "regr = joblib.load('model_dump/model_0.2.1.pkl')\n",
    "regr_version = '0.2.1'\n",
    "test_yhat = regr.predict(test_X)\n",
    "train_yhat = regr.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = np.sum((test_yhat - test_y)**2)/len(test_y)\n",
    "train_mse = np.sum((train_yhat - train_y)**2)/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(trials, port_size, available_loans, regr, regr_version, test, loan_npv_rois,\n",
    "                default_series):\n",
    "    results = {}\n",
    "    pct_default = {}\n",
    "    test_copy = test.copy()\n",
    "    for trial in tqdm_notebook(np.arange(trials)):\n",
    "        loan_ids = np.random.choice(\n",
    "            test_copy.index.values, available_loans, replace=False)\n",
    "        loans_to_pick_from = test_copy.loc[loan_ids, :]\n",
    "        scores = regr.predict(loans_to_pick_from)\n",
    "        scores_series = pd.Series(dict(zip(loan_ids, scores)))\n",
    "        scores_series.sort_values(ascending=False, inplace=True)\n",
    "        picks = scores_series[:900].index.values\n",
    "        results[trial] = loan_npv_rois.loc[picks, :].mean().to_dict()\n",
    "        pct_default[trial] = (default_series.loc[picks].sum()) / port_size\n",
    "    pct_default_series = pd.Series(pct_default)\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df['pct_def'] = pct_default_series\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per done with baseline models, say 3000 loans available\n",
    "# , pick 900 of them\n",
    "trials = 20000\n",
    "port_size = 900\n",
    "available_loans = 3000\n",
    "model_results = eval_models(trials, port_size, available_loans, regr, regr_version, test_X, loan_npv_rois, default_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = []\n",
    "for col in model_results.columns.values:\n",
    "    multi_index.append((str(col),regr_version))\n",
    "append_results = model_results.copy()\n",
    "append_results.columns = pd.MultiIndex.from_tuples(multi_index, names = ['discount_rate', 'model'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index_results = []\n",
    "for col in results.columns.values:\n",
    "    multi_index_results.append((str(col[0]), col[1]))\n",
    "results.columns = pd.MultiIndex.from_tuples(multi_index_results, names = ['discount_rate', 'model'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    results = results.join(append_results)\n",
    "except ValueError:\n",
    "    results.loc[:, (slice(None), slice('0.2.1','0.2.1'))] = append_results\n",
    "results.sort_index(axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.open()\n",
    "store['results'] = full_results\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
