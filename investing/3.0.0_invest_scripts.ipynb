{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:04:07.974245Z",
     "start_time": "2018-08-19T20:04:07.957862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moving emailing into invest scripts themselves, based off ethermine scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T20:04:39.945017Z",
     "start_time": "2018-08-19T20:04:28.377716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1kaowlmHw92ZcThz0Do0q6RcW3UhNaLsCF-19j8I2MCg',\n",
       " 'tableRange': 'Sheet1!A1:B1',\n",
       " 'updates': {'spreadsheetId': '1kaowlmHw92ZcThz0Do0q6RcW3UhNaLsCF-19j8I2MCg',\n",
       "  'updatedRange': 'Sheet1!A2:B2',\n",
       "  'updatedRows': 1,\n",
       "  'updatedColumns': 2,\n",
       "  'updatedCells': 2}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile invest_script.py #invest_script_instant.py\n",
    "# print('From DL Server, wait invest')\n",
    "import requests\n",
    "import json\n",
    "import lendingclub.account_info as acc_info\n",
    "import re\n",
    "from sklearn.externals import joblib\n",
    "# import lendingclub.dataprep_and_modeling.modeling_utils.data_prep_new as data_prep\n",
    "import lendingclub.investing.investing_utils as investing_utils\n",
    "from investing_utils import StandardScalerJustin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as math\n",
    "import torch\n",
    "import pickle as pickle\n",
    "import datetime\n",
    "import smtplib\n",
    "import gspread\n",
    "import google.auth\n",
    "from google.oauth2 import service_account\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "\n",
    "# constants and setup for various accounts and APIs\n",
    "now = datetime.datetime.now()\n",
    "token = acc_info.token\n",
    "inv_acc_id = acc_info.investor_id\n",
    "portfolio_id = acc_info.portfolio_id\n",
    "my_gmail_account = acc_info.from_email_throwaway\n",
    "my_gmail_password = acc_info.password_throwaway+'!@'\n",
    "my_recipients = acc_info.to_emails_throwaway\n",
    "header = {\n",
    "    'Authorization': token,\n",
    "    'Content-Type': 'application/json',\n",
    "    'X-LC-LISTING-VERSION': '1.3'\n",
    "}\n",
    "\n",
    "acc_summary_url = 'https://api.lendingclub.com/api/investor/v1/accounts/' + \\\n",
    "    str(inv_acc_id) + '/summary'\n",
    "order_url = 'https://api.lendingclub.com/api/investor/v1/accounts/' + \\\n",
    "    str(inv_acc_id) + '/orders'\n",
    "min_score = -0.02  # -0.04599714276994965  # -0.035764345824470828\n",
    "inv_amt = 25.00\n",
    "cash_limit = 0.00\n",
    "creds = service_account.Credentials.from_service_account_file(acc_info.project_path+'creds.json')\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "creds = creds.with_scopes(scope)\n",
    "gc = gspread.Client(auth=creds)\n",
    "gc.session = AuthorizedSession(creds)\n",
    "invest_ss_key = '1kaowlmHw92ZcThz0Do0q6RcW3UhNaLsCF-19j8I2MCg'\n",
    "investins_ss_key = '1AOXKQnxNf0ydTLElRUVoPLJ3jigZKZfRUq-iOlZcKN0'\n",
    "sheet = gc.open_by_key(invest_ss_key).sheet1\n",
    "sheetins = gc.open_by_key(investins_ss_key).sheet1\n",
    "\n",
    "# First check if I have enough money that I want to invest. min 10 notes so 250\n",
    "summary_dict = json.loads(requests.get(\n",
    "    acc_summary_url, headers=header).content)\n",
    "cash_to_invest = summary_dict['availableCash']\n",
    "\n",
    "# Load models and things for models\n",
    "# RF\n",
    "rf = investing_utils.load_RF()\n",
    "with open(f'{investing_utils.data_save_path}/for_proc_df_model_loading.pkl', 'rb') as handle:\n",
    "    nas_all_train, embeddings_all_train, train_cols_meds_all_train, use_cols, cols_all_train, col_cat_dict, mean_stdev_mapper_all_train, dl_df_train, dl_ys_train, cat_vars, emb_szs = pickle.load(handle)\n",
    "    \n",
    "# process the dataframe before I'm able to set up the neural net _____________\n",
    "# wait until it is time to do the api call. I'm rate limited to 1 call a second\n",
    "investing_utils.pause_until_time(test=True)\n",
    "\n",
    "# get the loans and process the dataframe\n",
    "_, all_loan_count = investing_utils.get_loans_and_ids(\n",
    "    header, exclude_already=False)\n",
    "api_loans, api_ids = investing_utils.get_loans_and_ids(\n",
    "    header, exclude_already=True)\n",
    "\n",
    "# cut api loans to cols that are cols I'll need\n",
    "api_ori_use_cols = [col for col in api_loans.columns if col in use_cols]\n",
    "api_loans = api_loans[api_ori_use_cols]\n",
    "api_loans['fake_ys'] = -999\n",
    "date_cols = ['earliest_cr_line', 'sec_app_earliest_cr_line']\n",
    "for col in date_cols:\n",
    "    api_loans[col] = pd.to_datetime(api_loans[col]).apply(lambda dt: dt.replace(day=1))\n",
    "investing_utils.add_dateparts(api_loans)    \n",
    "investing_utils.train_cats(api_loans)\n",
    "ordered_cat_cols = ['grade', 'sub_grade']\n",
    "for col in col_cat_dict.keys():\n",
    "    if col in ordered_cat_cols:\n",
    "        ordered = True\n",
    "    else:\n",
    "        ordered = False\n",
    "    api_loans[col] = pd.Categorical(api_loans[col], categories = col_cat_dict[col], ordered = ordered)\n",
    "X_test, y_test, nas, _, mean_stdev_mapper = investing_utils.proc_df_justin(api_loans, 'fake_ys', valid_test = True, do_scale=True, na_dict=nas_all_train, mapper = mean_stdev_mapper_all_train, train_cols_meds=train_cols_meds_all_train, cols=cols_all_train)\n",
    "# fake a last row for val_idxs for X_test and y_test\n",
    "fake_row = pd.DataFrame(X_test.shape[1]*[-999]).T\n",
    "fake_row.columns=X_test.columns\n",
    "X_test = X_test.append(fake_row)\n",
    "y_test = np.append(y_test, np.array([-999]))\n",
    "\n",
    "# setup NN and load saved weights\n",
    "md = investing_utils.ColumnarModelData.from_data_frame(investing_utils.PATH_NN, val_idxs=[len(X_test)-1], df=X_test, y=y_test, cat_flds=cat_vars, bs=1000, test_df=X_test.iloc[:-1,:])\n",
    "n_cont = len(dl_df_train.columns)-len(cat_vars)\n",
    "nn = md.get_learner(emb_szs, n_cont, 0.05, 1, [1000,500,500,250,250], [0.2,0.2,.2,.15,.05])\n",
    "nn.load(f'{investing_utils.PATH_NN}{investing_utils.regr_version_NN}_{investing_utils.training_type}.pth')\n",
    "\n",
    "# score the api_loans, filter to min score\n",
    "# net score\n",
    "nn_api_yhat = nn.predict(is_test=True)\n",
    "nn_api_yhat = nn_api_yhat.reshape(-1)\n",
    "# rf score\n",
    "rf_api_yhat = rf.predict(X_test.iloc[:-1,:])\n",
    "#combined score\n",
    "api_yhat = (nn_api_yhat + rf_api_yhat)/2\n",
    "\n",
    "# matching scores and loans\n",
    "ids_and_scores = pd.DataFrame(pd.Series(dict(zip(api_ids, api_yhat))))\n",
    "def get_preds(RF): return RF.predict(X_test.iloc[:-1,:])\n",
    "preds = np.stack(investing_utils.parallel_trees(rf, get_preds))\n",
    "# CIs = investing_utils.make_CIs(preds)\n",
    "ids_and_scores = pd.DataFrame(ids_and_scores)\n",
    "ids_and_scores.rename(columns={0:'3.0.0_score'}, inplace=True)\n",
    "# ids_and_scores['rf_mean'] = CIs['mean'].values\n",
    "# ids_and_scores['rf_std_dev'] = CIs['std_dev'].values\n",
    "ids_and_scores = ids_and_scores.sort_values('3.0.0_score',ascending=False)\n",
    "loans_to_pick_from = ids_and_scores[ids_and_scores['3.0.0_score'] >= min_score]\n",
    "loans_to_pick_from = loans_to_pick_from.sort_values('3.0.0_score', ascending=False)\n",
    "\n",
    "# See how many loans to pick from, set up order\n",
    "n_to_pick = int(math.floor(cash_to_invest / inv_amt))\n",
    "to_order_loan_ids = loans_to_pick_from[:n_to_pick].index.values\n",
    "orders_dict = {'aid': inv_acc_id}\n",
    "orders_list = []\n",
    "for loan_ids in to_order_loan_ids:\n",
    "    orders_list.append({'loanId': int(loan_ids),\n",
    "                        'requestedAmount': int(inv_amt),\n",
    "                        'portfolioId': int(portfolio_id)})\n",
    "orders_dict['orders'] = orders_list\n",
    "payload = json.dumps(orders_dict)\n",
    "if cash_to_invest >= cash_limit:\n",
    "    order_response = requests.post(order_url, headers=header, data=payload)\n",
    "else:\n",
    "    pass\n",
    "#     print('Cash to invest is ${0}. Waiting for at least ${1} cash before investing'.format(\n",
    "#         cash_to_invest, cash_limit))\n",
    "\n",
    "ids_and_scores.index.name = 'loan_id'    \n",
    "\n",
    "def send_emails():\n",
    "    subject = now.strftime(\"%Y-%m-%d %H:%M:%S.%f\") + ' Investment Round'\n",
    "    smtpserver = smtplib.SMTP('smtp.gmail.com',587)\n",
    "    smtpserver.ehlo()\n",
    "    smtpserver.starttls()\n",
    "    smtpserver.login(my_gmail_account, my_gmail_password)\n",
    "    message = '''\n",
    "Ran investment round.\n",
    "Cash to invest: ${0}, meaning {1} possible notes to invest in at ${2} each.\n",
    "{3} loans seen through api in total.\n",
    "{4} loans seen through api excluding already invested. \n",
    "{5} could be ordered due to score or cash available. Min score cutoff is {6}\n",
    "Response: {7}, {8}\n",
    "Scores from this batch:\n",
    "{9}\n",
    "    '''.format(cash_to_invest, n_to_pick, inv_amt, len(all_loan_count), len(api_loans), len(to_order_loan_ids), min_score, order_response, order_response.content, ids_and_scores)\n",
    "    msg = \"\"\"From: %s\\nTo: %s\\nSubject: %s\\n\\n%s\"\"\" % (my_gmail_account, my_recipients, subject, message)\n",
    "    smtpserver.sendmail(my_gmail_account, my_recipients, msg)\n",
    "    smtpserver.close()\n",
    "    \n",
    "    \n",
    "# send out the e-mails\n",
    "send_emails()\n",
    "\n",
    "# write some stats to a google spreadsheet\n",
    "# TODO from https://www.twilio.com/blog/2017/02/an-easy-way-to-read-and-write-to-a-google-spreadsheet-in-python.html\n",
    "sheet.append_row([now.strftime(\"%Y-%m-%d %H:%M:%S.%f\"), len(all_loan_count)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T03:39:26.831590Z",
     "start_time": "2018-03-24T03:39:26.806633Z"
    }
   },
   "source": [
    "# Writing out to investing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-18T02:20:14.862933Z",
     "start_time": "2018-08-18T02:20:14.828212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting investing_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile investing_utils.py\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import lendingclub.account_info as acc_info\n",
    "import pause\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class StandardScalerJustin(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "        self.copy = copy\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if type(X) == np.ndarray:\n",
    "            X = pd.Series(X.reshape(-1))\n",
    "        self.mean_ = X.dropna().mean()\n",
    "        self.var_ = X.dropna().var()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        mean = self.mean_\n",
    "        std_dev = np.sqrt(self.var_)\n",
    "        if std_dev == 0:\n",
    "            return X\n",
    "        return (X-mean)/std_dev\n",
    "    \n",
    "def fit_scalers(df, mapper):\n",
    "    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n",
    "    if mapper is None:\n",
    "        map_f = [([n],StandardScalerJustin()) for n in df.columns if is_numeric_dtype(df[n])]\n",
    "        mapper = DataFrameMapper(map_f).fit(df)\n",
    "    return mapper    \n",
    "\n",
    "def proc_df_justin(df, y_fld, valid_test, skip_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None, train_cols_meds=None, cols=None):\n",
    "\n",
    "    \"\"\" proc_df takes a data frame df and splits off the response variable, and\n",
    "    changes the df into an entirely numeric dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: The data frame you wish to process.\n",
    "\n",
    "    y_fld: The name of the response variable\n",
    "    \n",
    "    valid_test: boolean indicating if this is a df to match to train columns.\n",
    "\n",
    "    skip_flds: A list of fields that dropped from df.\n",
    "\n",
    "    do_scale: Standardizes each column in df,Takes Boolean Values(True,False)\n",
    "\n",
    "    na_dict: a dictionary of na columns to add. Na columns are also added if there\n",
    "        are any missing values.\n",
    "\n",
    "    preproc_fn: A function that gets applied to df.\n",
    "\n",
    "    max_n_cat: The maximum number of categories to break into dummy values, instead\n",
    "        of integer codes.\n",
    "\n",
    "    subset: Takes a random subset of size subset from df.\n",
    "\n",
    "    mapper: If do_scale is set as True, the mapper variable\n",
    "        calculates the values used for scaling of variables during training time(mean and standard deviation).\n",
    "        \n",
    "    train_cols_meds: dict where keys are columns from training and values are medians, use for values to fill an entire missing column (shouldn't be needed when used to actually pick loans, was needed for train/valid/test due to new fields being added over the timeframe and missing in certain datasets while existing in others)\n",
    "    \n",
    "    cols: Just to compare column order and ensure the variables are in the right order.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    [x, y, nas, mapper(optional)]:\n",
    "\n",
    "        x: x is the transformed version of df. x will not have the response variable\n",
    "            and is entirely numeric.\n",
    "\n",
    "        y: y is the response variable\n",
    "\n",
    "        nas: returns a dictionary of which nas it created, and the associated median.\n",
    "\n",
    "        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continous\n",
    "        variables which is then used for scaling of during test-time.\"\"\"        \n",
    "    assert type(valid_test) == bool, print('must indiciate if this is test/valid set to match columns with train')\n",
    "    \n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    df = df.copy()\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    y = df[y_fld].values\n",
    "    df.drop(skip_flds+[y_fld], axis=1, inplace=True)\n",
    "\n",
    "    # fit the scalers\n",
    "    if do_scale: mapper = fit_scalers(df, mapper)\n",
    "    if na_dict is None: na_dict = {}      \n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    df[mapper.transformed_names_] = mapper.transform(df)\n",
    "    embeddings=[]\n",
    "    for n,c in df.items():\n",
    "        numericalize(df, c, n, max_n_cat)\n",
    "        if not is_numeric_dtype(c):\n",
    "            embeddings.append(prep_embeddings(c, n))\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    # fix the nas\n",
    "    if valid_test:\n",
    "        for col, med in train_cols_meds.items():\n",
    "            try:\n",
    "                df[col].fillna(med, inplace=True)\n",
    "            except KeyError:\n",
    "                print(col)\n",
    "                df[col] = med\n",
    "        df = df[cols]\n",
    "        \n",
    "    res = [df, y, na_dict, embeddings]\n",
    "    if not valid_test: res += [res[0].median(), res[0].columns]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def prep_embeddings(c, n):\n",
    "    # allocate in embeddings for a null\n",
    "    return (n, len(c.cat.categories)+1)\n",
    "\n",
    "def eval_models(trials, port_size, available_loans, regr_version, X_test, y_test,\n",
    "                default_series, yhat_test): #regr, \n",
    "    results = {}\n",
    "    pct_default = {}\n",
    "    test_copy = X_test.copy()\n",
    "    \n",
    "    default_series = default_series.loc[X_test.index]\n",
    "    yhats_ys_defs = pd.DataFrame([yhat_test, y_test, default_series.values]).T\n",
    "    yhats_ys_defs.rename(columns={0:'yhat', 1:'y', 2:'defaults'}, inplace=True)\n",
    "    for trial in tqdm_notebook(np.arange(trials)):\n",
    "        # of all test loans, grab a batch of n=available_loans\n",
    "        available_idx = np.random.choice(\n",
    "            np.arange(len(test_copy)), available_loans, replace=False)\n",
    "        available_loans_df = yhats_ys_defs.ix[available_idx,:]\n",
    "        available_loans_df.sort_values('yhat', inplace=True, ascending=False)\n",
    "        picks = available_loans_df[:port_size]\n",
    "        results[trial] = picks['y'].mean()\n",
    "        pct_default[trial] = picks['defaults'].sum()/port_size\n",
    "    pct_default_series = pd.Series(pct_default)\n",
    "    results_df = pd.DataFrame(pd.Series(results))\n",
    "    results_df['pct_def'] = pct_default_series\n",
    "    results_df.columns = pd.MultiIndex(levels=[[regr_version], [0.07, 'pct_def']],\n",
    "           labels=[[0, 0,], [0, 1,]],\n",
    "           names=['discount_rate', 'model'])\n",
    "    return results_df\n",
    "\n",
    "def load_RF():\n",
    "    return joblib.load(f'{PATH_RF}{regr_version_RF}_{training_type}.pkl')\n",
    "    \n",
    "def add_dateparts(df):\n",
    "    '''Uses the fastai add_datepart to turn datetimes into numbers to process\n",
    "       does not do it for issue_d'''\n",
    "    date_cols = df.select_dtypes(['datetime64']).columns\n",
    "    for date_col in date_cols:\n",
    "        if date_col not in special_cols:\n",
    "            add_datepart(df, date_col, drop=True)\n",
    "    return [col for col in date_cols if col not in special_cols]    \n",
    "\n",
    "def pause_until_time(test=False):\n",
    "    # pause 3 seconds, then print hello world\n",
    "    now = dt.datetime.now()\n",
    "    current_hour = now.hour\n",
    "    if not test:\n",
    "        pause_until = dt.datetime(\n",
    "            now.year, now.month, now.day, now.hour + 1, 0, 0)\n",
    "    if test:\n",
    "        # if testing, wait 2 seconds and print('will pause 2 seconds')\n",
    "        pause_until = dt.datetime(\n",
    "            now.year, now.month, now.day, now.hour, now.minute, now.second + 2)\n",
    "#         print('will pause 2 seconds')\n",
    "#     print('right now it is {0}, pausing until {1}'.format(\n",
    "#         now.strftime('%H:%M:%S'), pause_until.strftime('%H:%M:%S')))\n",
    "    pause.until(pause_until)\n",
    "\n",
    "\n",
    "def convert_to_underscore(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([0-9A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "def get_already_invested_filter_id(header):\n",
    "    filters_list = json.loads(requests.get(\n",
    "        'https://api.lendingclub.com/api/investor/v1/accounts/' + str(inv_acc_id) + '/filters', headers=header).content)\n",
    "    filters_df = pd.DataFrame(filters_list['filters'])\n",
    "    # I manually made a single filter that excludes loans already invested in.\n",
    "    # Not sure if there is a way to do this entirely through the api.\n",
    "    return filters_df[filters_df['name'] == 'exclude_already_invested'].iloc[0, 0]\n",
    "\n",
    "\n",
    "def get_loans_and_ids(header, exclude_already=True):\n",
    "    '''Gets loans from lendingclub with the single filter of exclude loans already invested in.'''\n",
    "    if exclude_already:\n",
    "        filter_id = get_already_invested_filter_id(header)\n",
    "        payload = {'showAll': 'true', 'filterId': filter_id}\n",
    "        resp = requests.get(\n",
    "            'https://api.lendingclub.com/api/investor/v1/loans/listing', headers=header, params=payload) #'https://api.lendingclub.com/api/investor/v1/loans/listing'\n",
    "        loans_list = json.loads(resp.content)['loans']\n",
    "    if not exclude_already:\n",
    "        payload = {'showAll': 'true'}\n",
    "        resp = requests.get(\n",
    "            'https://api.lendingclub.com/api/investor/v1/loans/listing', headers=header, params=payload)\n",
    "        loans_list = json.loads(resp.content)['loans']\n",
    "\n",
    "    api_loans = pd.DataFrame(loans_list)\n",
    "    api_loans.columns = np.array(\n",
    "        [convert_to_underscore(col) for col in api_loans.columns.values])\n",
    "    # save the loan ids\n",
    "    loan_ids = api_loans['id']\n",
    "    return api_loans, loan_ids\n",
    "\n",
    "\n",
    "def match_col_names(api_loans):\n",
    "    # cols to add\n",
    "    # make a col of nans so cols match up exactly\n",
    "    api_loans['issue_d'] = 0\n",
    "    api_loans['line_history_m'] = 0\n",
    "    api_loans['maturity_paid'] = 0\n",
    "    api_loans['maturity_time'] = 0\n",
    "    api_loans['npv_roi_10'] = 0\n",
    "    api_loans['orig_amt_due'] = 0\n",
    "    api_loans['target_loose'] = 0\n",
    "    api_loans['target_strict'] = 0\n",
    "    api_loans['fico'] = 0\n",
    "\n",
    "    cols_to_drop_immediately = [\n",
    "        'accept_d',\n",
    "        'credit_pull_d',\n",
    "        'desc',\n",
    "        'emp_title',\n",
    "        'exp_d',\n",
    "        'exp_default_rate',\n",
    "        'funded_amount',\n",
    "        'housing_payment',\n",
    "        'id',\n",
    "        'ils_exp_d',\n",
    "        'initial_list_status',\n",
    "        'investor_count',\n",
    "        'list_d',\n",
    "        'member_id',\n",
    "        'mtg_payment',\n",
    "        'review_status',\n",
    "        'review_status_d',\n",
    "        'sec_app_earliest_cr_line',\n",
    "        'sec_app_fico_range_high',\n",
    "        'sec_app_fico_range_low',\n",
    "        'service_fee_rate',\n",
    "    ]\n",
    "    api_loans.drop(cols_to_drop_immediately, axis=1, inplace=True)\n",
    "    rename_dict = {\n",
    "        'acc_open_past_24_mths': 'acc_open_past_24mths',\n",
    "        'addr_zip': 'zip_code',\n",
    "        'delinq_2_yrs': 'delinq_2yrs',\n",
    "        'i_l_util': 'il_util',\n",
    "        'inq_last_6_mths': 'inq_last_6mths',\n",
    "        'installment': 'installment_amount',\n",
    "        'is_inc_v': 'verification_status',\n",
    "        'is_inc_v_joint': 'verification_status_joint',\n",
    "        'loan_amount': 'loan_amnt',\n",
    "        'num_accts_ever_12_0_ppd': 'num_accts_ever_120_pd',\n",
    "        'num_tl_12_0dpd_2m': 'num_tl_120dpd_2m',\n",
    "        'sec_app_inq_last_6_mths': 'sec_app_inq_last_6mths',\n",
    "    }\n",
    "    api_loans.rename(columns=rename_dict, inplace=True)\n",
    "    return api_loans\n",
    "\n",
    "\n",
    "def match_existing_cols_to_csv(api_loans):\n",
    "    api_loans.fillna(value=np.nan, inplace=True)\n",
    "    api_loans['all_util'] = api_loans['all_util'] / 100.0\n",
    "    api_loans['application_type'] = api_loans['application_type'].str.lower()\n",
    "\n",
    "    # turn employment length into categorical\n",
    "    emp_len_dict = {np.nan: 'n/a',\n",
    "                    0.0: '< 1 year',\n",
    "                    12.0: '1 year',\n",
    "                    24.0: '2 years',\n",
    "                    36.0: '3 years',\n",
    "                    48.0: '4 years',\n",
    "                    60.0: '5 years',\n",
    "                    72.0: '6 years',\n",
    "                    84.0: '7 years',\n",
    "                    96.0: '8 years',\n",
    "                    108.0: '9 years',\n",
    "                    120.0: '10+ years', }\n",
    "    api_loans['emp_length'] = api_loans['emp_length'].replace(emp_len_dict)\n",
    "    api_loans['home_ownership'] = api_loans['home_ownership'].str.lower()\n",
    "    api_loans['int_rate'] = api_loans['int_rate'] / 100.0\n",
    "\n",
    "    # verification status\n",
    "    dic_veri_status = {'NOT_VERIFIED': 'none',\n",
    "                       'SOURCE_VERIFIED': 'source',\n",
    "                       'VERIFIED': 'platform'}\n",
    "    api_loans['verification_status'] = api_loans[\n",
    "        'verification_status'].replace(dic_veri_status)\n",
    "    api_loans['verification_status_joint'] = api_loans[\n",
    "        'verification_status_joint'].replace(dic_veri_status)\n",
    "    api_loans['pct_tl_nvr_dlq'] = api_loans['pct_tl_nvr_dlq'] / 100.0\n",
    "    api_loans['percent_bc_gt_75'] = api_loans['percent_bc_gt_75'] / 100.0\n",
    "    api_loans['revol_util'] = api_loans['revol_util'] / 100.0\n",
    "    return api_loans\n",
    "\n",
    "\n",
    "def make_missing_cols_and_del_dates(api_loans):\n",
    "    # probably something with earliest credit line, fico range high/low\n",
    "    # need to add line_history_m, orig_amt_due, fico\n",
    "    api_loans['fico'] = (api_loans['fico_range_high'] +\n",
    "                         api_loans['fico_range_low']) / 2\n",
    "    # line_history_m depends on issue_d, which doesn't exist for listed loans.\n",
    "    # Assume it takes one month to issue so increase the number compared to\n",
    "    # the csvs by 1\n",
    "    today = pd.to_datetime(dt.date.today())\n",
    "    api_loans['earliest_cr_line'] = pd.to_datetime(\n",
    "        api_loans['earliest_cr_line'])\n",
    "    line_hist_d = (today - api_loans['earliest_cr_line']) / np.timedelta64(\n",
    "        1, 'D')\n",
    "    api_loans['line_history_m'] = (line_hist_d * (12 / 365.25)).round() + 1\n",
    "    api_loans['orig_amt_due'] = api_loans[\n",
    "        'term'] * api_loans['installment_amount']\n",
    "\n",
    "    api_loans.drop(['earliest_cr_line', 'fico_range_high',\n",
    "                    'fico_range_low'], axis=1, inplace=True)\n",
    "    return api_loans\n",
    "\n",
    "\n",
    "def verify_df_base_cols(api_loans, test_loans):\n",
    "    api_cols = api_loans.columns.values.copy()\n",
    "    api_cols.sort()\n",
    "    csv_cols = test_loans.columns.values.copy()\n",
    "    csv_cols.sort()\n",
    "    assert len(api_cols) == len(csv_cols)\n",
    "    examine = dict(zip(api_cols, csv_cols))\n",
    "    for key, val in examine.iteritems():\n",
    "        if key != val:\n",
    "            print(key, val)\n",
    "            return None\n",
    "    return True\n",
    "\n",
    "def make_CIs(preds):\n",
    "    means = np.mean(preds, axis=0)\n",
    "    std_devs = np.std(preds, axis=0)\n",
    "    df = pd.DataFrame(np.zeros((preds.shape[1],2)), columns=['mean', 'std_dev'])\n",
    "    df['mean'] = means\n",
    "    df['std_dev'] = std_devs\n",
    "    return df\n",
    "\n",
    "# constants\n",
    "inv_acc_id = acc_info.investor_id\n",
    "special_cols = []\n",
    "platform = 'lendingclub'\n",
    "datapath = '/home/justin/all_data/'\n",
    "PATH_NN = f'{datapath}{platform}/NN/'\n",
    "PATH_RF = f'{datapath}{platform}/RF/'\n",
    "data_save_path = f'{datapath}{platform}/'\n",
    "training_type = 'all'\n",
    "regr_version_RF = '0.2.2'\n",
    "regr_version_NN = '1.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
