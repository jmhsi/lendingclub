{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:56:57.726093Z",
     "start_time": "2019-10-18T00:56:57.701388Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# testing\n",
    "from pandas.testing import assert_frame_equal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import j_utils.munging as mg\n",
    "from lendingclub.lc_utils import gen_datasets\n",
    "from lendingclub import config\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 60\n",
    "pd.options.display.max_seq_items = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the train script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input should be model type (should try to accept list)\n",
    "bundled with the parameters for each type of model\n",
    "\n",
    "should return/create the saved model and anything necessary for data processing\n",
    "for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T02:45:45.676870Z",
     "start_time": "2019-10-18T02:45:45.668149Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../lendingclub/modeling/08_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../lendingclub/modeling/08_train.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lendingclub import config, utils\n",
    "import j_utils.munging as mg\n",
    "\n",
    "def prepare_data(model_n, data, proc=None, ds_type='train'):\n",
    "    '''\n",
    "    returns the processed data for a model, which could be different between\n",
    "    model types e.g. can handle categoricals or not. additionally returns\n",
    "    a tuple of anything necessary to process valid/test data in the same manner\n",
    "    ds_type must be 'train', 'valid', or 'test'\n",
    "    '''\n",
    "    assert ds_type in ['train', 'valid', 'test'], print('ds_type invalid')\n",
    "    if model_n in ['baseline', 'A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        return data, None\n",
    "#     elif model_n == 'logistic_regr':\n",
    "    else:\n",
    "        if ds_type == 'train':\n",
    "            temp = mg.train_proc(data)\n",
    "            procced = temp[0]\n",
    "            return procced, temp[1:]\n",
    "        elif ds_type in ['test', 'valid']:\n",
    "            assert proc, print('must pass data processing artifacts')\n",
    "            temp = mg.val_test_proc(data, *proc)\n",
    "            return temp\n",
    "\n",
    "    \n",
    "def train_model(model_n, X_train, y_train, X_valid=None, y_valid=None):\n",
    "    if model_n in ['baseline', 'A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        return 42\n",
    "    elif model_n == 'logistic_regr':\n",
    "        lr_model = LogisticRegression(class_weight='balanced')\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        return lr_model\n",
    "    \n",
    "def export_models(m, model_n):\n",
    "    if model_n in ['baseline', 'A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        with open(os.path.join(config.modeling_dir, '{0}_model.pkl'.format(model_n)), 'wb') as file:\n",
    "            pickle.dump(m, file)\n",
    "    elif model_n == 'logistic_regr':\n",
    "        joblib.dump(m,os.path.join(config.modeling_dir, '{0}_model.pkl'.format(model_n)))\n",
    "    \n",
    "def export_data_processing(proc_arti, model_n):\n",
    "    if model_n in ['baseline', 'A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        with open(os.path.join(config.modeling_dir, '{0}_model_proc_arti.pkl'.format(model_n)), 'wb') as file:\n",
    "            pickle.dump(proc_arti, file)\n",
    "    elif model_n == 'logistic_regr':\n",
    "        joblib.dump(proc_arti, os.path.join(config.modeling_dir, '{0}_model_proc_arti.pkl'.format(model_n)))\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', '-m', help='specify model(s) to train')\n",
    "\n",
    "if not len(sys.argv) > 1:\n",
    "    models = ['logistic_regr'] # , 'A', 'B', 'C', 'D', 'E', 'F', 'G'\n",
    "\n",
    "args = parser.parse_args()\n",
    "if args.model:\n",
    "    models = args.model.split()\n",
    "# models = ['logistic_regr']\n",
    "\n",
    "\n",
    "if not os.path.isdir(config.modeling_dir):\n",
    "    os.makedirs(config.modeling_dir)\n",
    "\n",
    "tr_val_base_data, tr_val_eval_data, _ = utils.load_dataset(ds_type='train')\n",
    "# ensure ordering is correct for time series split\n",
    "tr_val_base_data, tr_val_eval_data = mg.sort_train_eval(tr_val_base_data, tr_val_eval_data, 'id', 'issue_d')\n",
    "\n",
    "\n",
    "for model_n in models:\n",
    "    # do 3 steps of TS cross validation, with valid size at 5% (20 splits)\n",
    "    tscv = mg.time_series_data_split(tr_val_eval_data, 'issue_d', 20, 1)\n",
    "    for tr_idx, val_idx in tscv:\n",
    "        # split out validation from train_data\n",
    "        y_train = tr_val_eval_data.loc[tr_idx, 'target_loose']\n",
    "        y_valid = tr_val_eval_data.loc[val_idx, 'target_loose']\n",
    "        X_train = tr_val_base_data.loc[tr_idx]\n",
    "        X_valid = tr_val_base_data.loc[val_idx]\n",
    "        \n",
    "        X_train, proc_arti = prepare_data(model_n, X_train, ds_type='train')\n",
    "        X_valid = prepare_data(model_n, X_valid, proc = proc_arti, ds_type='valid')\n",
    "        m = train_model(model_n, X_train, y_train)\n",
    "\n",
    "        #save stuff\n",
    "        export_models(m, model_n)\n",
    "        export_data_processing(proc_arti, model_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T01:35:09.097338Z",
     "start_time": "2019-10-18T01:12:35.100243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/envs/lendingclub/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "m = train_model(model_n, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T02:28:16.516904Z",
     "start_time": "2019-10-18T02:28:16.512148Z"
    }
   },
   "outputs": [],
   "source": [
    "export_models(m, model_n)\n",
    "export_data_processing(proc_arti, model_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copied code to incorporate into train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T09:56:22.277095Z",
     "start_time": "2019-10-17T09:56:22.271356Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from lendingclub import config\n",
    "import pickle\n",
    "dpath = config.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T05:05:24.363517Z",
     "start_time": "2019-10-17T05:05:24.360617Z"
    }
   },
   "outputs": [],
   "source": [
    "# from lendingclub.lc_utils import gen_datasets\n",
    "from j_utils import munging as mg\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T09:53:55.882922Z",
     "start_time": "2019-10-17T09:53:55.739238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_loan_info.fth                     dev_ids.pkl\r\n",
      "bootstrap_test_eval_loan_info_ids.pkl  eval_loan_info.fth\r\n",
      "bootstrap_test_idx.pkl                 eval_loan_info_scored.fth\r\n",
      "clean_loan_info_api_name_matched.fth   raw_loan_info.fth\r\n",
      "clean_loan_info.fth                    scaled_pmt_hist.fth\r\n",
      "clean_pmt_history.fth                  strings_loan_info.fth\r\n",
      "\u001b[0m\u001b[01;34mcsvs\u001b[0m/                                  train_test_ids.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls {dpath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T18:29:41.875273Z",
     "start_time": "2019-10-17T18:29:40.125180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2507335, 90) (2507335, 45) 1447630\n"
     ]
    }
   ],
   "source": [
    "base_loan_info = pd.read_feather(os.path.join(dpath, 'base_loan_info.fth'))\n",
    "eval_loan_info = pd.read_feather(os.path.join(dpath, 'eval_loan_info.fth'))\n",
    "with open(os.path.join(dpath, 'train_test_ids.pkl'), 'rb') as f:\n",
    "    train_test_ids = pickle.load(f)\n",
    "    \n",
    "use_ids = train_test_ids['train']\n",
    "\n",
    "print(base_loan_info.shape, eval_loan_info.shape, len(use_ids))\n",
    "\n",
    "tv_base_loan_info = base_loan_info.query('id in @use_ids')\n",
    "tv_eval_loan_info = eval_loan_info.query('id in @use_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T05:15:27.352760Z",
     "start_time": "2019-10-17T05:15:27.311922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12434, 90) (12434, 44)\n"
     ]
    }
   ],
   "source": [
    "tt_base_loan_info = pd.read_feather(os.path.join(dpath, 'train_testable_base_loan_info.fth'))\n",
    "tt_eval_loan_info = pd.read_feather(os.path.join(dpath, 'train_testable_eval_loan_info.fth'))\n",
    "print(tt_base_loan_info.shape, tt_eval_loan_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_loan_info, train_eval_loan_info['target_strict'], test_size=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai style processing\n",
    "X_train, all_train_colnames, max_dict, min_dict, new_null_colnames, fill_dict, cats_dict, norm_dict = mg.train_proc(X_train)\n",
    "X_valid = mg.val_test_proc(X_valid, all_train_colnames, max_dict, min_dict, fill_dict, cats_dict, norm_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lendingclub]",
   "language": "python",
   "name": "conda-env-lendingclub-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
